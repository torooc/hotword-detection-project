{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "# import pydub\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "from scipy import signal as scipy_signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import datetime\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/home/torooc/dataHDD2/speech_commands_v0.01/'\n",
    "\n",
    "run_name = 'runs/crnn_log_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_list_path = dataset_dir + 'testing_list.txt'\n",
    "validation_list_path = dataset_dir + 'validation_list.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(testing_list_path) as f:\n",
    "    testing_list = f.readlines()\n",
    "    testing_list = [file.strip() for file in testing_list]\n",
    "    \n",
    "with open(validation_list_path) as f:\n",
    "    validation_list = f.readlines()\n",
    "    validation_list = [file.strip() for file in validation_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_dirs = os.listdir(dataset_dir)\n",
    "\n",
    "command_dirs = [direc for direc in command_dirs if os.path.isdir(os.path.join(dataset_dir, direc))]\n",
    "\n",
    "command_dirs.sort()\n",
    "\n",
    "command_dirs.remove('_background_noise_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go', 'happy', 'house', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up', 'wow', 'yes', 'zero']\n"
     ]
    }
   ],
   "source": [
    "print(command_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "key2label_dict = dict()\n",
    "label2key_list = list()\n",
    "\n",
    "key2label_dict[' '] = 0\n",
    "label2key_list.append(' ')\n",
    "\n",
    "for i, command in enumerate(command_dirs):\n",
    "    key2label_dict[command] = i + 1\n",
    "    label2key_list.append(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'bed': 1,\n",
       " 'bird': 2,\n",
       " 'cat': 3,\n",
       " 'dog': 4,\n",
       " 'down': 5,\n",
       " 'eight': 6,\n",
       " 'five': 7,\n",
       " 'four': 8,\n",
       " 'go': 9,\n",
       " 'happy': 10,\n",
       " 'house': 11,\n",
       " 'left': 12,\n",
       " 'marvin': 13,\n",
       " 'nine': 14,\n",
       " 'no': 15,\n",
       " 'off': 16,\n",
       " 'on': 17,\n",
       " 'one': 18,\n",
       " 'right': 19,\n",
       " 'seven': 20,\n",
       " 'sheila': 21,\n",
       " 'six': 22,\n",
       " 'stop': 23,\n",
       " 'three': 24,\n",
       " 'tree': 25,\n",
       " 'two': 26,\n",
       " 'up': 27,\n",
       " 'wow': 28,\n",
       " 'yes': 29,\n",
       " 'zero': 30}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key2label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_wav_file_list = list()\n",
    "\n",
    "for command_dir in command_dirs:\n",
    "    wav_list = os.listdir(os.path.join(dataset_dir, command_dir))\n",
    "    \n",
    "    wav_list_with_prefix = [os.path.join(command_dir, file_name) for file_name in wav_list]\n",
    "    \n",
    "    total_wav_file_list += wav_list_with_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list = list(set(total_wav_file_list) - set(testing_list) - set(validation_list))\n",
    "training_list.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvalidation_list\\ntesting_list\\ntraining_list\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "validation_list\n",
    "testing_list\n",
    "training_list\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audio_metadata = list()\n",
    "\n",
    "for wav_file_name in total_wav_file_list:\n",
    "    audio = AudioSegment.from_wav(dataset_dir + wav_file_name)\n",
    "    fs = audio.frame_rate\n",
    "    length_in_samples = audio.frame_count()\n",
    "    \n",
    "#   normalized_audio_array = np.asarray(audio.get_array_of_samples()) / 2 ** 15\n",
    "    \n",
    "#   print(\"Max: {}, Min: {}\".format(max(normalized_audio_array), min(normalized_audio_array)))\n",
    "    \n",
    "    audio_metadata.append([fs, length_in_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 16000\n",
    "\n",
    "class DatasetLoader():\n",
    "    \n",
    "    def __init__(self, batch_size, dataset_dir, wav_file_name_list, key2label, label2key):\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.wav_file_name_list = copy.deepcopy(wav_file_name_list)\n",
    "        self.sr = SR\n",
    "        self.nsc_in_ms = 40\n",
    "#         self.nov_in_ms = self.nsc_in_ms / 2\n",
    "        self.nov_in_ms = 0\n",
    "        self.nsc_in_sample = int(self.nsc_in_ms / 1000 * self.sr)\n",
    "        self.nov_in_sample = int(self.nov_in_ms / 1000 * self.sr)\n",
    "        \n",
    "        self.key2label = key2label\n",
    "        self.label2key = label2key\n",
    "        \n",
    "        self.spectrogram_list = list()\n",
    "        self.label_list = list()\n",
    "        \n",
    "        self.lock = threading.RLock()\n",
    "        \n",
    "        self.num_thread = 8\n",
    "        \n",
    "        self.loading_counter = 0\n",
    "        \n",
    "        self.dataset_number = len(self.wav_file_name_list)\n",
    "        \n",
    "    def shuffle_dataset_order(self):\n",
    "        random.shuffle(self.wav_file_name_list)\n",
    "        \n",
    "    def load_dataset(self):\n",
    "        \n",
    "        self.shuffle_dataset_order()\n",
    "        self.spectrogram_list = list()\n",
    "        self.label_list = list()\n",
    "        self.loading_counter = 0\n",
    "        \n",
    "        step = int(np.floor(self.dataset_number / self.num_thread))\n",
    "        \n",
    "        for i in range(self.num_thread):\n",
    "            \n",
    "            wav_file_name_list_thread = self.wav_file_name_list[i*step:(i+1)*step]\n",
    "            \n",
    "            if i + 1 == self.num_thread:\n",
    "                wav_file_name_list_thread = self.wav_file_name_list[i*step:]\n",
    "            \n",
    "            thread = threading.Thread(target=self.load_spectrogram, args=(wav_file_name_list_thread,))\n",
    "            thread.start()\n",
    "        \n",
    "    def batch_generator(self):\n",
    "        \n",
    "        self.load_dataset()\n",
    "        \n",
    "        while self.loading_counter < self.dataset_number:\n",
    "            \n",
    "            if len(self.spectrogram_list) >= self.batch_size:\n",
    "\n",
    "                spectrogram_list = self.spectrogram_list[:self.batch_size]\n",
    "                label_list = self.label_list[:self.batch_size]\n",
    "\n",
    "                with self.lock:\n",
    "                    self.spectrogram_list = self.spectrogram_list[self.batch_size:]\n",
    "                    self.label_list = self.label_list[self.batch_size:]\n",
    "\n",
    "                spectrogram_time_step_list = [specgram.shape[1] for specgram in spectrogram_list]\n",
    "                max_time_step = max(spectrogram_time_step_list)\n",
    "                freq_size = spectrogram_list[0].shape[0]\n",
    "                batch = np.zeros([len(spectrogram_list), freq_size, max_time_step])\n",
    "\n",
    "                for j, specgram in enumerate(spectrogram_list):\n",
    "                    batch[j, :specgram.shape[0], :specgram.shape[1]] = specgram\n",
    "\n",
    "                batch_label = np.asarray(label_list)\n",
    "\n",
    "                yield batch, batch_label\n",
    "            \n",
    "            time.sleep(1)\n",
    "        \n",
    "        \n",
    "        if len(self.spectrogram_list) > 0:\n",
    "\n",
    "            spectrogram_list = self.spectrogram_list[:self.batch_size]\n",
    "            label_list = self.label_list[:self.batch_size]\n",
    "\n",
    "            with self.lock:\n",
    "                self.spectrogram_list = self.spectrogram_list[self.batch_size:]\n",
    "                self.label_list = self.label_list[self.batch_size:]\n",
    "\n",
    "            spectrogram_time_step_list = [specgram.shape[1] for specgram in spectrogram_list]\n",
    "            max_time_step = max(spectrogram_time_step_list)\n",
    "            freq_size = spectrogram_list[0].shape[0]\n",
    "            batch = np.zeros([len(spectrogram_list), freq_size, max_time_step])\n",
    "\n",
    "            for j, specgram in enumerate(spectrogram_list):\n",
    "                batch[j, :specgram.shape[0], :specgram.shape[1]] = specgram\n",
    "\n",
    "            batch_label = np.asarray(label_list)\n",
    "\n",
    "            yield batch, batch_label\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "\n",
    "    def load_spectrogram(self, wav_file_name_list_part):\n",
    "\n",
    "        for i, wav_file_name in enumerate(wav_file_name_list_part):\n",
    "\n",
    "            audio = AudioSegment.from_wav(self.dataset_dir + wav_file_name)\n",
    "\n",
    "            normalized_audio_array = np.asarray(audio.get_array_of_samples()) / 2 ** 15\n",
    "\n",
    "            f, t, Zxx = scipy_signal.stft(normalized_audio_array, fs=self.sr, \n",
    "                                          nperseg=self.nsc_in_sample,\n",
    "                                          noverlap=self.nov_in_sample)\n",
    "\n",
    "            Sxx = np.abs(Zxx)\n",
    "\n",
    "            normalized_spectrogram = (20 * np.log10(np.maximum(Sxx, 1e-8)) + 160) / 160\n",
    "            \n",
    "            keyword = wav_file_name.split('/')[0]\n",
    "            label = self.key2label[keyword]\n",
    "            \n",
    "            with self.lock:\n",
    "                self.spectrogram_list.append(normalized_spectrogram)\n",
    "                self.label_list.append(label)\n",
    "                self.loading_counter += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51088"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader_training = DatasetLoader(256, dataset_dir, training_list, key2label_dict, label2key_list)\n",
    "dataset_loader_testing = DatasetLoader(256, dataset_dir, testing_list, key2label_dict, label2key_list)\n",
    "dataset_loader_validation = DatasetLoader(256, dataset_dir, validation_list, key2label_dict, label2key_list)\n",
    "\n",
    "# dataset_loader_testing = DatasetLoader(64, dataset_dir, testing_list, key2label_dict, label2key_list)\n",
    "# dataset_loader_validation = DatasetLoader(64, dataset_dir, validation_list, key2label_dict, label2key_list)\n",
    "\n",
    "# dataset_loader_training = DatasetLoader(768, dataset_dir, training_list, key2label_dict, label2key_list)\n",
    "# dataset_loader_testing = DatasetLoader(768, dataset_dir, testing_list, key2label_dict, label2key_list)\n",
    "# dataset_loader_validation = DatasetLoader(768, dataset_dir, validation_list, key2label_dict, label2key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_generator = dataset_loader_training.batch_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class basic_model(nn.Module):\n",
    "#     def __init__(self, D_in, H, num_layers, num_labels):\n",
    "#         super(basic_model, self).__init__()\n",
    "#         self.fc = torch.nn.Linear(D_in, H)\n",
    "#         self.relu = torch.nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "#         self.gru_layers = nn.ModuleList([nn.GRU(H, int(H / 2), bidirectional=True, batch_first=True) for i in range(num_layers)])\n",
    "\n",
    "#         self.fc_pred = nn.Linear(H, num_labels)\n",
    "#         self.log_softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "#     def forward(self, input_tensor):\n",
    "#         # (B, T, F)\n",
    "#         output_tensor = self.fc(input_tensor)\n",
    "#         output_tensor = self.relu(output_tensor)\n",
    "#         output_tensor = self.dropout(output_tensor)\n",
    "#         # (B, T, H)\n",
    "#         for layer in self.gru_layers:\n",
    "#             output_tensor, _ = layer(output_tensor)\n",
    "            \n",
    "#         output_tensor = self.fc_pred(output_tensor)\n",
    "\n",
    "#         output_tensor = self.log_softmax(output_tensor)\n",
    "        \n",
    "#         return output_tensor\n",
    "\n",
    "class basic_conv_model(nn.Module):\n",
    "#     def __init__(self, first_kernel_size, second_kernel_size, dropout_rate, num_labels):\n",
    "    def __init__(self):\n",
    "        super(basic_conv_model, self).__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(1, 16, (13, 2))\n",
    "        self.conv2d_2 = nn.Conv2d(16, 32, (13, 2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(4800, 31)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        input_tensor.unsqueeze_(1)\n",
    "        tensor = self.conv2d_1(input_tensor)\n",
    "#         print('[After 1st Conv2d]: {}'.format(tensor.shape))\n",
    "        tensor = F.relu(tensor)\n",
    "        tensor = F.dropout(tensor, 0.1, training=True)\n",
    "        tensor = F.max_pool2d(tensor, (3, 2))\n",
    "#         print('[After 1st maxpool2d]: {}'.format(tensor.shape))\n",
    "        tensor = self.conv2d_2(tensor)\n",
    "#         print('[After 2nd Conv2d]: {}'.format(tensor.shape))\n",
    "        tensor = F.relu(tensor)\n",
    "        tensor = F.dropout(tensor, 0.1, training=True)\n",
    "        tensor = F.max_pool2d(tensor, (3, 2))\n",
    "#         print('[After 2nd maxpool2d]: {}'.format(tensor.shape))\n",
    "        tensor = self.flatten(tensor)\n",
    "#         print('[After Flatten]: {}'.format(tensor.shape))\n",
    "        tensor = self.fc(tensor)\n",
    "#         print('[After fc]: {}'.format(tensor.shape))\n",
    "        pred_tensor = F.log_softmax(tensor, dim=-1)\n",
    "        \n",
    "        return pred_tensor\n",
    "    \n",
    "class basic_crnn_model(nn.Module):\n",
    "#     def __init__(self, first_kernel_size, second_kernel_size, dropout_rate, num_labels):\n",
    "    def __init__(self):\n",
    "        super(basic_crnn_model, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(1, 16, (13, 2))\n",
    "        self.gru_1 = nn.GRU(16 * 103, 128, 1)\n",
    "        self.gru_2 = nn.GRU(128, 128, 1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(128 * 12, 31)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        input_tensor.unsqueeze_(1)  # (B, 1, F, T)\n",
    "        tensor = self.conv2d(input_tensor) # (B, 16, F, T)\n",
    "#         print('[After 1st Conv2d]: {}'.format(tensor.shape))\n",
    "        tensor = F.relu(tensor)\n",
    "        tensor = F.dropout(tensor, 0.1, training=True)\n",
    "        tensor = F.max_pool2d(tensor, (3, 2))\n",
    "#         print('[After 1st maxpool2d]: {}'.format(tensor.shape))\n",
    "        \n",
    "        tensor = tensor.view(tensor.shape[0], -1, tensor.shape[3])\n",
    "        # (B, 16 * F, T)\n",
    "        \n",
    "#         print('[After Reshape]: {}'.format(tensor.shape))\n",
    "        \n",
    "        tensor.transpose_(0, 2) # (T, F, B)\n",
    "        tensor.transpose_(1, 2) # (T, B, F)\n",
    "\n",
    "        tensor, _ = self.gru_1(tensor)\n",
    "#         print('[After 2nd Conv2d]: {}'.format(tensor.shape))\n",
    "        tensor = F.relu(tensor)\n",
    "        tensor = F.dropout(tensor, 0.1, training=True) # (T, B, F)\n",
    "#         tensor = F.max_pool2d(tensor, (3, 2))\n",
    "#         print('[After 2nd maxpool2d]: {}'.format(tensor.shape))\n",
    "\n",
    "        tensor, _ = self.gru_2(tensor)\n",
    "#         print('[After 2nd Conv2d]: {}'.format(tensor.shape))\n",
    "        tensor = F.relu(tensor)\n",
    "        tensor = F.dropout(tensor, 0.1, training=True) # (T, B, F)\n",
    "\n",
    "        tensor.transpose_(0, 1) # (B, T, F)\n",
    "    \n",
    "#         print('[After Reshape]: {}'.format(tensor.shape))\n",
    "    \n",
    "        tensor = self.flatten(tensor)\n",
    "#         print('[After Flatten]: {}'.format(tensor.shape))\n",
    "        tensor = self.fc(tensor)\n",
    "#         print('[After fc]: {}'.format(tensor.shape))\n",
    "        pred_tensor = F.log_softmax(tensor, dim=-1)\n",
    "        \n",
    "        return pred_tensor\n",
    "\n",
    "    \n",
    "# model = basic_model(321, 512, 3, 31).float().to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# dictation_loss = nn.CTCLoss().to(device)\n",
    "\n",
    "model = basic_crnn_model().float().to(device)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_generator = dataset_loader_training.batch_generator()\n",
    "# next(batch_generator)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:04:38.744045\n",
      "#000 [Train: 3.419] [Test: 3.398] [Valid: 3.398]\n",
      "#000 [Train: 0.038] [Test: 0.044] [Valid: 0.045]\n",
      "12:05:22.413778\n",
      "#001 [Train: 3.402] [Test: 3.393] [Valid: 3.396]\n",
      "#001 [Train: 0.039] [Test: 0.033] [Valid: 0.041]\n",
      "12:06:05.990448\n",
      "#002 [Train: 3.393] [Test: 3.380] [Valid: 3.386]\n",
      "#002 [Train: 0.047] [Test: 0.057] [Valid: 0.059]\n",
      "12:06:49.561584\n",
      "#003 [Train: 3.373] [Test: 3.350] [Valid: 3.351]\n",
      "#003 [Train: 0.071] [Test: 0.084] [Valid: 0.076]\n",
      "12:07:33.189335\n",
      "#004 [Train: 3.325] [Test: 3.277] [Valid: 3.275]\n",
      "#004 [Train: 0.082] [Test: 0.096] [Valid: 0.090]\n",
      "12:08:17.862646\n",
      "#005 [Train: 3.225] [Test: 3.173] [Valid: 3.133]\n",
      "#005 [Train: 0.095] [Test: 0.098] [Valid: 0.127]\n",
      "12:09:02.569878\n",
      "#006 [Train: 3.117] [Test: 3.052] [Valid: 3.072]\n",
      "#006 [Train: 0.119] [Test: 0.128] [Valid: 0.135]\n",
      "12:09:47.329668\n",
      "#007 [Train: 3.055] [Test: 3.037] [Valid: 2.984]\n",
      "#007 [Train: 0.126] [Test: 0.130] [Valid: 0.137]\n",
      "12:10:32.109845\n",
      "#008 [Train: 3.009] [Test: 3.010] [Valid: 2.987]\n",
      "#008 [Train: 0.128] [Test: 0.124] [Valid: 0.136]\n",
      "12:11:16.861514\n",
      "#009 [Train: 2.965] [Test: 2.919] [Valid: 2.898]\n",
      "#009 [Train: 0.138] [Test: 0.148] [Valid: 0.171]\n",
      "12:12:01.577579\n",
      "#010 [Train: 2.915] [Test: 2.889] [Valid: 2.875]\n",
      "#010 [Train: 0.144] [Test: 0.163] [Valid: 0.163]\n",
      "12:12:46.299259\n",
      "#011 [Train: 2.874] [Test: 2.839] [Valid: 2.798]\n",
      "#011 [Train: 0.162] [Test: 0.164] [Valid: 0.183]\n",
      "12:13:31.022379\n",
      "#012 [Train: 2.818] [Test: 2.839] [Valid: 2.724]\n",
      "#012 [Train: 0.175] [Test: 0.185] [Valid: 0.212]\n",
      "12:14:15.795434\n",
      "#013 [Train: 2.772] [Test: 2.757] [Valid: 2.683]\n",
      "#013 [Train: 0.186] [Test: 0.177] [Valid: 0.209]\n",
      "12:14:59.413541\n",
      "#014 [Train: 2.729] [Test: 2.751] [Valid: 2.660]\n",
      "#014 [Train: 0.197] [Test: 0.195] [Valid: 0.227]\n",
      "12:15:44.179517\n",
      "#015 [Train: 2.701] [Test: 2.707] [Valid: 2.605]\n",
      "#015 [Train: 0.204] [Test: 0.209] [Valid: 0.226]\n",
      "12:16:25.648899\n",
      "#016 [Train: 2.659] [Test: 2.619] [Valid: 2.617]\n",
      "#016 [Train: 0.213] [Test: 0.236] [Valid: 0.249]\n",
      "12:17:04.612447\n",
      "#017 [Train: 2.618] [Test: 2.745] [Valid: 2.560]\n",
      "#017 [Train: 0.228] [Test: 0.221] [Valid: 0.237]\n",
      "12:17:45.828623\n",
      "#018 [Train: 2.603] [Test: 2.570] [Valid: 2.550]\n",
      "#018 [Train: 0.234] [Test: 0.254] [Valid: 0.270]\n",
      "12:18:26.055673\n",
      "#019 [Train: 2.545] [Test: 2.581] [Valid: 2.424]\n",
      "#019 [Train: 0.243] [Test: 0.230] [Valid: 0.285]\n",
      "12:19:04.988966\n",
      "#020 [Train: 2.525] [Test: 2.523] [Valid: 2.488]\n",
      "#020 [Train: 0.260] [Test: 0.235] [Valid: 0.272]\n",
      "12:19:44.009301\n",
      "#021 [Train: 2.470] [Test: 2.493] [Valid: 2.402]\n",
      "#021 [Train: 0.273] [Test: 0.259] [Valid: 0.282]\n",
      "12:20:26.551977\n",
      "#022 [Train: 2.454] [Test: 2.496] [Valid: 2.393]\n",
      "#022 [Train: 0.275] [Test: 0.273] [Valid: 0.298]\n",
      "12:21:03.290988\n",
      "#023 [Train: 2.428] [Test: 2.511] [Valid: 2.379]\n",
      "#023 [Train: 0.278] [Test: 0.271] [Valid: 0.290]\n",
      "12:21:42.389716\n",
      "#024 [Train: 2.436] [Test: 2.477] [Valid: 2.348]\n",
      "#024 [Train: 0.280] [Test: 0.287] [Valid: 0.310]\n",
      "12:22:25.960256\n",
      "#025 [Train: 2.381] [Test: 2.482] [Valid: 2.434]\n",
      "#025 [Train: 0.290] [Test: 0.270] [Valid: 0.284]\n",
      "12:23:10.699603\n",
      "#026 [Train: 2.374] [Test: 2.443] [Valid: 2.300]\n",
      "#026 [Train: 0.297] [Test: 0.288] [Valid: 0.313]\n",
      "12:23:55.420786\n",
      "#027 [Train: 2.318] [Test: 2.344] [Valid: 2.303]\n",
      "#027 [Train: 0.309] [Test: 0.326] [Valid: 0.334]\n",
      "12:24:40.198796\n",
      "#028 [Train: 2.302] [Test: 2.342] [Valid: 2.244]\n",
      "#028 [Train: 0.326] [Test: 0.304] [Valid: 0.345]\n",
      "12:25:24.948135\n",
      "#029 [Train: 2.292] [Test: 2.346] [Valid: 2.260]\n",
      "#029 [Train: 0.326] [Test: 0.305] [Valid: 0.328]\n",
      "12:26:04.102754\n",
      "#030 [Train: 2.281] [Test: 2.299] [Valid: 2.237]\n",
      "#030 [Train: 0.326] [Test: 0.307] [Valid: 0.345]\n",
      "12:26:43.077921\n",
      "#031 [Train: 2.239] [Test: 2.223] [Valid: 2.182]\n",
      "#031 [Train: 0.335] [Test: 0.339] [Valid: 0.343]\n",
      "12:27:26.681546\n",
      "#032 [Train: 2.236] [Test: 2.237] [Valid: 2.173]\n",
      "#032 [Train: 0.351] [Test: 0.334] [Valid: 0.359]\n",
      "12:28:10.297517\n",
      "#033 [Train: 2.198] [Test: 2.260] [Valid: 2.178]\n",
      "#033 [Train: 0.353] [Test: 0.316] [Valid: 0.348]\n",
      "12:28:53.951683\n",
      "#034 [Train: 2.172] [Test: 2.211] [Valid: 2.167]\n",
      "#034 [Train: 0.359] [Test: 0.370] [Valid: 0.355]\n",
      "12:29:36.478321\n",
      "#035 [Train: 2.147] [Test: 2.157] [Valid: 2.091]\n",
      "#035 [Train: 0.365] [Test: 0.370] [Valid: 0.395]\n",
      "12:30:20.007999\n",
      "#036 [Train: 2.137] [Test: 2.122] [Valid: 2.107]\n",
      "#036 [Train: 0.375] [Test: 0.393] [Valid: 0.380]\n",
      "12:30:57.881341\n",
      "#037 [Train: 2.094] [Test: 2.234] [Valid: 2.099]\n",
      "#037 [Train: 0.383] [Test: 0.369] [Valid: 0.386]\n",
      "12:31:41.491290\n",
      "#038 [Train: 2.096] [Test: 2.238] [Valid: 2.133]\n",
      "#038 [Train: 0.388] [Test: 0.345] [Valid: 0.358]\n",
      "12:32:24.014600\n",
      "#039 [Train: 2.079] [Test: 2.097] [Valid: 2.003]\n",
      "#039 [Train: 0.383] [Test: 0.376] [Valid: 0.405]\n",
      "12:33:07.634139\n",
      "#040 [Train: 2.019] [Test: 2.079] [Valid: 1.972]\n",
      "#040 [Train: 0.407] [Test: 0.402] [Valid: 0.420]\n",
      "12:33:51.226053\n",
      "#041 [Train: 2.002] [Test: 2.108] [Valid: 2.014]\n",
      "#041 [Train: 0.415] [Test: 0.375] [Valid: 0.401]\n",
      "12:34:33.624452\n",
      "#042 [Train: 2.007] [Test: 1.999] [Valid: 1.987]\n",
      "#042 [Train: 0.410] [Test: 0.406] [Valid: 0.422]\n",
      "12:35:12.586455\n",
      "#043 [Train: 1.964] [Test: 1.975] [Valid: 1.891]\n",
      "#043 [Train: 0.427] [Test: 0.415] [Valid: 0.451]\n",
      "12:35:54.995071\n",
      "#044 [Train: 1.923] [Test: 2.037] [Valid: 1.967]\n",
      "#044 [Train: 0.433] [Test: 0.406] [Valid: 0.426]\n",
      "12:36:38.532882\n",
      "#045 [Train: 1.950] [Test: 1.932] [Valid: 1.877]\n",
      "#045 [Train: 0.435] [Test: 0.446] [Valid: 0.452]\n",
      "12:37:22.057652\n",
      "#046 [Train: 1.889] [Test: 1.959] [Valid: 1.906]\n",
      "#046 [Train: 0.442] [Test: 0.418] [Valid: 0.427]\n",
      "12:38:05.523385\n",
      "#047 [Train: 1.869] [Test: 1.956] [Valid: 1.864]\n",
      "#047 [Train: 0.454] [Test: 0.442] [Valid: 0.430]\n",
      "12:38:46.802571\n",
      "#048 [Train: 1.864] [Test: 1.939] [Valid: 1.879]\n",
      "#048 [Train: 0.462] [Test: 0.435] [Valid: 0.436]\n",
      "12:39:27.059023\n",
      "#049 [Train: 1.840] [Test: 1.871] [Valid: 1.808]\n",
      "#049 [Train: 0.453] [Test: 0.452] [Valid: 0.469]\n",
      "12:40:08.242900\n",
      "#050 [Train: 1.812] [Test: 1.879] [Valid: 1.828]\n",
      "#050 [Train: 0.466] [Test: 0.451] [Valid: 0.477]\n",
      "12:40:50.695302\n",
      "#051 [Train: 1.789] [Test: 1.842] [Valid: 1.779]\n",
      "#051 [Train: 0.476] [Test: 0.452] [Valid: 0.491]\n",
      "12:41:34.264878\n",
      "#052 [Train: 1.760] [Test: 1.859] [Valid: 1.737]\n",
      "#052 [Train: 0.485] [Test: 0.451] [Valid: 0.495]\n",
      "12:42:17.816860\n",
      "#053 [Train: 1.760] [Test: 1.753] [Valid: 1.709]\n",
      "#053 [Train: 0.483] [Test: 0.504] [Valid: 0.502]\n",
      "12:43:00.235461\n",
      "#054 [Train: 1.733] [Test: 1.787] [Valid: 1.743]\n",
      "#054 [Train: 0.494] [Test: 0.488] [Valid: 0.493]\n",
      "12:43:41.485677\n",
      "#055 [Train: 1.713] [Test: 1.751] [Valid: 1.726]\n",
      "#055 [Train: 0.500] [Test: 0.462] [Valid: 0.474]\n",
      "12:44:20.466657\n",
      "#056 [Train: 1.673] [Test: 1.733] [Valid: 1.670]\n",
      "#056 [Train: 0.511] [Test: 0.492] [Valid: 0.511]\n",
      "12:45:04.027462\n",
      "#057 [Train: 1.660] [Test: 1.806] [Valid: 1.645]\n",
      "#057 [Train: 0.511] [Test: 0.466] [Valid: 0.511]\n",
      "12:45:47.628093\n",
      "#058 [Train: 1.660] [Test: 1.671] [Valid: 1.647]\n",
      "#058 [Train: 0.516] [Test: 0.515] [Valid: 0.515]\n",
      "12:46:30.036866\n",
      "#059 [Train: 1.624] [Test: 1.734] [Valid: 1.639]\n",
      "#059 [Train: 0.517] [Test: 0.506] [Valid: 0.527]\n",
      "12:47:13.587515\n",
      "#060 [Train: 1.626] [Test: 1.693] [Valid: 1.604]\n",
      "#060 [Train: 0.521] [Test: 0.501] [Valid: 0.522]\n",
      "12:47:56.071466\n",
      "#061 [Train: 1.592] [Test: 1.656] [Valid: 1.594]\n",
      "#061 [Train: 0.533] [Test: 0.513] [Valid: 0.531]\n",
      "12:48:33.821956\n",
      "#062 [Train: 1.563] [Test: 1.673] [Valid: 1.618]\n",
      "#062 [Train: 0.548] [Test: 0.514] [Valid: 0.522]\n",
      "12:49:17.424173\n",
      "#063 [Train: 1.553] [Test: 1.652] [Valid: 1.569]\n",
      "#063 [Train: 0.548] [Test: 0.530] [Valid: 0.557]\n",
      "12:50:01.009546\n",
      "#064 [Train: 1.566] [Test: 1.649] [Valid: 1.558]\n",
      "#064 [Train: 0.540] [Test: 0.522] [Valid: 0.540]\n",
      "12:50:43.446646\n",
      "#065 [Train: 1.523] [Test: 1.629] [Valid: 1.497]\n",
      "#065 [Train: 0.552] [Test: 0.532] [Valid: 0.553]\n",
      "12:51:25.859360\n",
      "#066 [Train: 1.521] [Test: 1.555] [Valid: 1.503]\n",
      "#066 [Train: 0.552] [Test: 0.559] [Valid: 0.553]\n",
      "12:52:08.309880\n",
      "#067 [Train: 1.505] [Test: 1.536] [Valid: 1.512]\n",
      "#067 [Train: 0.561] [Test: 0.559] [Valid: 0.560]\n",
      "12:52:47.276799\n",
      "#068 [Train: 1.482] [Test: 1.576] [Valid: 1.467]\n",
      "#068 [Train: 0.571] [Test: 0.534] [Valid: 0.567]\n",
      "12:53:29.717526\n",
      "#069 [Train: 1.485] [Test: 1.556] [Valid: 1.470]\n",
      "#069 [Train: 0.562] [Test: 0.548] [Valid: 0.578]\n",
      "12:54:13.320612\n",
      "#070 [Train: 1.436] [Test: 1.526] [Valid: 1.484]\n",
      "#070 [Train: 0.582] [Test: 0.569] [Valid: 0.562]\n",
      "12:54:55.731825\n",
      "#071 [Train: 1.457] [Test: 1.459] [Valid: 1.405]\n",
      "#071 [Train: 0.575] [Test: 0.561] [Valid: 0.593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:55:39.324700\n",
      "#072 [Train: 1.426] [Test: 1.486] [Valid: 1.396]\n",
      "#072 [Train: 0.583] [Test: 0.572] [Valid: 0.591]\n",
      "12:56:20.732149\n",
      "#073 [Train: 1.410] [Test: 1.487] [Valid: 1.406]\n",
      "#073 [Train: 0.587] [Test: 0.570] [Valid: 0.613]\n",
      "12:57:00.844941\n",
      "#074 [Train: 1.385] [Test: 1.421] [Valid: 1.409]\n",
      "#074 [Train: 0.597] [Test: 0.584] [Valid: 0.570]\n",
      "12:57:42.017189\n",
      "#075 [Train: 1.388] [Test: 1.488] [Valid: 1.385]\n",
      "#075 [Train: 0.594] [Test: 0.560] [Valid: 0.605]\n",
      "12:58:24.501003\n",
      "#076 [Train: 1.365] [Test: 1.471] [Valid: 1.367]\n",
      "#076 [Train: 0.601] [Test: 0.569] [Valid: 0.620]\n",
      "12:59:08.096935\n",
      "#077 [Train: 1.341] [Test: 1.463] [Valid: 1.330]\n",
      "#077 [Train: 0.613] [Test: 0.574] [Valid: 0.626]\n",
      "12:59:51.688440\n",
      "#078 [Train: 1.335] [Test: 1.409] [Valid: 1.337]\n",
      "#078 [Train: 0.614] [Test: 0.584] [Valid: 0.609]\n",
      "13:00:34.078468\n",
      "#079 [Train: 1.329] [Test: 1.432] [Valid: 1.349]\n",
      "#079 [Train: 0.621] [Test: 0.591] [Valid: 0.610]\n",
      "13:01:14.359734\n",
      "#080 [Train: 1.319] [Test: 1.364] [Valid: 1.294]\n",
      "#080 [Train: 0.615] [Test: 0.603] [Valid: 0.614]\n",
      "13:01:54.371515\n",
      "#081 [Train: 1.350] [Test: 1.357] [Valid: 1.345]\n",
      "#081 [Train: 0.603] [Test: 0.611] [Valid: 0.616]\n",
      "13:02:36.887208\n",
      "#082 [Train: 1.297] [Test: 1.354] [Valid: 1.279]\n",
      "#082 [Train: 0.616] [Test: 0.615] [Valid: 0.634]\n",
      "13:03:20.454973\n",
      "#083 [Train: 1.281] [Test: 1.331] [Valid: 1.266]\n",
      "#083 [Train: 0.630] [Test: 0.620] [Valid: 0.616]\n",
      "13:03:55.156561\n",
      "#084 [Train: 1.275] [Test: 1.303] [Valid: 1.363]\n",
      "#084 [Train: 0.633] [Test: 0.625] [Valid: 0.595]\n",
      "13:04:23.138718\n",
      "#085 [Train: 1.286] [Test: 1.332] [Valid: 1.349]\n",
      "#085 [Train: 0.633] [Test: 0.607] [Valid: 0.602]\n",
      "13:04:51.116315\n",
      "#086 [Train: 1.260] [Test: 1.392] [Valid: 1.256]\n",
      "#086 [Train: 0.637] [Test: 0.576] [Valid: 0.643]\n",
      "13:05:18.952594\n",
      "#087 [Train: 1.226] [Test: 1.416] [Valid: 1.317]\n",
      "#087 [Train: 0.648] [Test: 0.565] [Valid: 0.620]\n",
      "13:05:46.687220\n",
      "#088 [Train: 1.225] [Test: 1.290] [Valid: 1.286]\n",
      "#088 [Train: 0.652] [Test: 0.629] [Valid: 0.639]\n",
      "13:06:13.495687\n",
      "#089 [Train: 1.210] [Test: 1.315] [Valid: 1.255]\n",
      "#089 [Train: 0.652] [Test: 0.620] [Valid: 0.622]\n",
      "13:06:41.356587\n",
      "#090 [Train: 1.236] [Test: 1.310] [Valid: 1.245]\n",
      "#090 [Train: 0.636] [Test: 0.590] [Valid: 0.628]\n",
      "13:07:09.247889\n",
      "#091 [Train: 1.220] [Test: 1.285] [Valid: 1.247]\n",
      "#091 [Train: 0.649] [Test: 0.634] [Valid: 0.635]\n",
      "13:07:37.052551\n",
      "#092 [Train: 1.240] [Test: 1.327] [Valid: 1.331]\n",
      "#092 [Train: 0.644] [Test: 0.609] [Valid: 0.626]\n",
      "13:08:04.868985\n",
      "#093 [Train: 1.195] [Test: 1.238] [Valid: 1.215]\n",
      "#093 [Train: 0.652] [Test: 0.645] [Valid: 0.659]\n",
      "13:08:32.723558\n",
      "#094 [Train: 1.184] [Test: 1.270] [Valid: 1.212]\n",
      "#094 [Train: 0.659] [Test: 0.629] [Valid: 0.641]\n",
      "13:09:00.539948\n",
      "#095 [Train: 1.162] [Test: 1.243] [Valid: 1.168]\n",
      "#095 [Train: 0.655] [Test: 0.635] [Valid: 0.673]\n",
      "13:09:36.122037\n",
      "#096 [Train: 1.179] [Test: 1.239] [Valid: 1.221]\n",
      "#096 [Train: 0.656] [Test: 0.645] [Valid: 0.641]\n",
      "13:10:12.851326\n",
      "#097 [Train: 1.173] [Test: 1.214] [Valid: 1.202]\n",
      "#097 [Train: 0.654] [Test: 0.646] [Valid: 0.647]\n",
      "13:10:49.696846\n",
      "#098 [Train: 1.171] [Test: 1.264] [Valid: 1.205]\n",
      "#098 [Train: 0.662] [Test: 0.627] [Valid: 0.625]\n",
      "13:11:26.438719\n",
      "#099 [Train: 1.135] [Test: 1.201] [Valid: 1.194]\n",
      "#099 [Train: 0.666] [Test: 0.641] [Valid: 0.649]\n",
      "13:12:03.083414\n",
      "#100 [Train: 1.151] [Test: 1.263] [Valid: 1.170]\n",
      "#100 [Train: 0.662] [Test: 0.624] [Valid: 0.664]\n",
      "13:12:32.029601\n",
      "#101 [Train: 1.131] [Test: 1.170] [Valid: 1.140]\n",
      "#101 [Train: 0.675] [Test: 0.659] [Valid: 0.672]\n",
      "13:12:59.819593\n",
      "#102 [Train: 1.135] [Test: 1.253] [Valid: 1.163]\n",
      "#102 [Train: 0.669] [Test: 0.652] [Valid: 0.646]\n",
      "13:13:27.637342\n",
      "#103 [Train: 1.153] [Test: 1.297] [Valid: 1.143]\n",
      "#103 [Train: 0.671] [Test: 0.617] [Valid: 0.669]\n",
      "13:13:55.556730\n",
      "#104 [Train: 1.163] [Test: 1.258] [Valid: 1.120]\n",
      "#104 [Train: 0.667] [Test: 0.622] [Valid: 0.676]\n",
      "13:14:23.311150\n",
      "#105 [Train: 1.154] [Test: 1.212] [Valid: 1.144]\n",
      "#105 [Train: 0.666] [Test: 0.659] [Valid: 0.664]\n",
      "13:14:51.136884\n",
      "#106 [Train: 1.079] [Test: 1.175] [Valid: 1.104]\n",
      "#106 [Train: 0.683] [Test: 0.654] [Valid: 0.674]\n",
      "13:15:18.961377\n",
      "#107 [Train: 1.117] [Test: 1.224] [Valid: 1.162]\n",
      "#107 [Train: 0.681] [Test: 0.663] [Valid: 0.661]\n",
      "13:15:46.848605\n",
      "#108 [Train: 1.045] [Test: 1.191] [Valid: 1.165]\n",
      "#108 [Train: 0.695] [Test: 0.665] [Valid: 0.681]\n",
      "13:16:14.672583\n",
      "#109 [Train: 1.063] [Test: 1.249] [Valid: 1.139]\n",
      "#109 [Train: 0.690] [Test: 0.622] [Valid: 0.654]\n",
      "13:16:42.502024\n",
      "#110 [Train: 1.102] [Test: 1.180] [Valid: 1.041]\n",
      "#110 [Train: 0.679] [Test: 0.654] [Valid: 0.698]\n",
      "13:17:10.416671\n",
      "#111 [Train: 1.086] [Test: 1.192] [Valid: 1.170]\n",
      "#111 [Train: 0.684] [Test: 0.650] [Valid: 0.669]\n",
      "13:17:38.196702\n",
      "#112 [Train: 1.092] [Test: 1.205] [Valid: 1.035]\n",
      "#112 [Train: 0.681] [Test: 0.663] [Valid: 0.715]\n",
      "13:18:06.039023\n",
      "#113 [Train: 1.093] [Test: 1.188] [Valid: 1.081]\n",
      "#113 [Train: 0.682] [Test: 0.650] [Valid: 0.694]\n",
      "13:18:30.528193\n",
      "#114 [Train: 1.066] [Test: 1.113] [Valid: 1.140]\n",
      "#114 [Train: 0.688] [Test: 0.660] [Valid: 0.660]\n",
      "13:18:55.030139\n",
      "#115 [Train: 1.089] [Test: 1.163] [Valid: 1.079]\n",
      "#115 [Train: 0.678] [Test: 0.684] [Valid: 0.699]\n",
      "13:19:19.557165\n",
      "#116 [Train: 1.063] [Test: 1.126] [Valid: 1.036]\n",
      "#116 [Train: 0.692] [Test: 0.663] [Valid: 0.710]\n",
      "13:19:44.136381\n",
      "#117 [Train: 1.073] [Test: 1.133] [Valid: 1.138]\n",
      "#117 [Train: 0.693] [Test: 0.665] [Valid: 0.671]\n",
      "13:20:08.608290\n",
      "#118 [Train: 1.061] [Test: 1.142] [Valid: 1.031]\n",
      "#118 [Train: 0.692] [Test: 0.661] [Valid: 0.695]\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCH = 300\n",
    "\n",
    "writer = SummaryWriter(log_dir=run_name)\n",
    "\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    training_loss_list = list()\n",
    "    batch_generator = dataset_loader_training.batch_generator()\n",
    "    model.train()\n",
    "    \n",
    "    correct_num = 0\n",
    "    total_num = 0\n",
    "\n",
    "    for (batch, label) in batch_generator:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(torch.cuda.FloatTensor(batch))\n",
    "        loss = F.nll_loss(pred, torch.cuda.LongTensor(label))\n",
    "        loss.backward()\n",
    "        training_loss_list.append(loss.item())\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, pred_label = torch.max(pred, 1)\n",
    "        correct_num += int((pred_label == torch.cuda.LongTensor(label)).sum().cpu().numpy())\n",
    "        total_num += len(label)\n",
    "        \n",
    "    acc_train = correct_num / total_num\n",
    "\n",
    "    testing_loss_list = list()\n",
    "\n",
    "    batch_generator = dataset_loader_testing.batch_generator()\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    correct_num = 0\n",
    "    total_num = 0\n",
    "\n",
    "    for (batch, label) in batch_generator:\n",
    "    #     optimizer.zero_grad()\n",
    "        pred = model(torch.cuda.FloatTensor(batch))\n",
    "        loss = F.nll_loss(pred, torch.cuda.LongTensor(label))\n",
    "#         loss.backward()\n",
    "        testing_loss_list.append(loss.item())\n",
    "    #     optimizer.step()\n",
    "    \n",
    "        _, pred_label = torch.max(pred, 1)\n",
    "        correct_num += int((pred_label == torch.cuda.LongTensor(label)).sum().cpu().numpy())\n",
    "        total_num += len(label)\n",
    "        \n",
    "    acc_test = correct_num / total_num\n",
    "\n",
    "    validation_loss_list = list()\n",
    "\n",
    "    batch_generator = dataset_loader_validation.batch_generator()\n",
    "    \n",
    "    correct_num = 0\n",
    "    total_num = 0\n",
    "\n",
    "    for (batch, label) in batch_generator:\n",
    "    #     optimizer.zero_grad()\n",
    "        pred = model(torch.cuda.FloatTensor(batch))\n",
    "        loss = F.nll_loss(pred, torch.cuda.LongTensor(label))\n",
    "#         loss.backward()\n",
    "        validation_loss_list.append(loss.item())\n",
    "    #     optimizer.step()\n",
    "    \n",
    "        _, pred_label = torch.max(pred, 1)\n",
    "        correct_num += int((pred_label == torch.cuda.LongTensor(label)).sum().cpu().numpy())\n",
    "        total_num += len(label)\n",
    "        \n",
    "    acc_valid = correct_num / total_num\n",
    "    \n",
    "    training_loss_mean = np.mean(training_loss_list)\n",
    "    testing_loss_mean = np.mean(testing_loss_list)\n",
    "    validation_loss_mean = np.mean(validation_loss_list)\n",
    "    \n",
    "    print(datetime.datetime.now().time())\n",
    "    print('#{:03d} [Train: {:0.3f}] [Test: {:0.3f}] [Valid: {:0.3f}]'.format(epoch, training_loss_mean, testing_loss_mean, validation_loss_mean))\n",
    "    print('#{:03d} [Train: {:0.3f}] [Test: {:0.3f}] [Valid: {:0.3f}]'.format(epoch, acc_train, acc_test, acc_valid))\n",
    "    \n",
    "    writer.add_scalar('Loss/Train', training_loss_mean, epoch)\n",
    "    writer.add_scalar('Loss/Test', testing_loss_mean, epoch)\n",
    "    writer.add_scalar('Loss/Valid', validation_loss_mean, epoch)\n",
    "    writer.add_scalar('Acc/Train', acc_train, epoch)\n",
    "    writer.add_scalar('Acc/Test', acc_test, epoch)\n",
    "    writer.add_scalar('Acc/Valid', acc_valid, epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model(torch.cuda.FloatTensor(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor(batch).unsqueeze_(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor = torch.tensor(batch).to(device).float()\n",
    "# input_tensor = input_tensor.transpose(1, 2)\n",
    "# model(input_tensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for batch, label_list in batch_generator:\n",
    "#     print(batch.shape[0])\n",
    "#     plt.figure(figsize=(5, 1))\n",
    "#     plt.plot(label_list)\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.figure(figsize=(6, 18))\n",
    "# #     plt.imshow(batch[0], aspect='auto')\n",
    "#     plt.imshow(batch[0])\n",
    "#     plt.show()\n",
    "    \n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
