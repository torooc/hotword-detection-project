{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "# import pydub\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "from scipy import signal as scipy_signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import datetime\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/home/torooc/dataHDD2/speech_commands_v0.01/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_list_path = dataset_dir + 'testing_list.txt'\n",
    "validation_list_path = dataset_dir + 'validation_list.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(testing_list_path) as f:\n",
    "    testing_list = f.readlines()\n",
    "    testing_list = [file.strip() for file in testing_list]\n",
    "    \n",
    "with open(validation_list_path) as f:\n",
    "    validation_list = f.readlines()\n",
    "    validation_list = [file.strip() for file in validation_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_dirs = os.listdir(dataset_dir)\n",
    "\n",
    "command_dirs = [direc for direc in command_dirs if os.path.isdir(os.path.join(dataset_dir, direc))]\n",
    "\n",
    "command_dirs.sort()\n",
    "\n",
    "command_dirs.remove('_background_noise_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go', 'happy', 'house', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up', 'wow', 'yes', 'zero']\n"
     ]
    }
   ],
   "source": [
    "print(command_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "key2label_dict = dict()\n",
    "label2key_list = list()\n",
    "\n",
    "key2label_dict[' '] = 0\n",
    "label2key_list.append(' ')\n",
    "\n",
    "for i, command in enumerate(command_dirs):\n",
    "    key2label_dict[command] = i + 1\n",
    "    label2key_list.append(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'bed': 1,\n",
       " 'bird': 2,\n",
       " 'cat': 3,\n",
       " 'dog': 4,\n",
       " 'down': 5,\n",
       " 'eight': 6,\n",
       " 'five': 7,\n",
       " 'four': 8,\n",
       " 'go': 9,\n",
       " 'happy': 10,\n",
       " 'house': 11,\n",
       " 'left': 12,\n",
       " 'marvin': 13,\n",
       " 'nine': 14,\n",
       " 'no': 15,\n",
       " 'off': 16,\n",
       " 'on': 17,\n",
       " 'one': 18,\n",
       " 'right': 19,\n",
       " 'seven': 20,\n",
       " 'sheila': 21,\n",
       " 'six': 22,\n",
       " 'stop': 23,\n",
       " 'three': 24,\n",
       " 'tree': 25,\n",
       " 'two': 26,\n",
       " 'up': 27,\n",
       " 'wow': 28,\n",
       " 'yes': 29,\n",
       " 'zero': 30}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key2label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_wav_file_list = list()\n",
    "\n",
    "for command_dir in command_dirs:\n",
    "    wav_list = os.listdir(os.path.join(dataset_dir, command_dir))\n",
    "    \n",
    "    wav_list_with_prefix = [os.path.join(command_dir, file_name) for file_name in wav_list]\n",
    "    \n",
    "    total_wav_file_list += wav_list_with_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list = list(set(total_wav_file_list) - set(testing_list) - set(validation_list))\n",
    "training_list.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvalidation_list\\ntesting_list\\ntraining_list\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "validation_list\n",
    "testing_list\n",
    "training_list\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audio_metadata = list()\n",
    "\n",
    "for wav_file_name in total_wav_file_list:\n",
    "    audio = AudioSegment.from_wav(dataset_dir + wav_file_name)\n",
    "    fs = audio.frame_rate\n",
    "    length_in_samples = audio.frame_count()\n",
    "    \n",
    "#   normalized_audio_array = np.asarray(audio.get_array_of_samples()) / 2 ** 15\n",
    "    \n",
    "#   print(\"Max: {}, Min: {}\".format(max(normalized_audio_array), min(normalized_audio_array)))\n",
    "    \n",
    "    audio_metadata.append([fs, length_in_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 16000\n",
    "\n",
    "class DatasetLoader():\n",
    "    \n",
    "    def __init__(self, batch_size, dataset_dir, wav_file_name_list, key2label, label2key):\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.wav_file_name_list = copy.deepcopy(wav_file_name_list)\n",
    "        self.sr = SR\n",
    "        self.nsc_in_ms = 40\n",
    "#         self.nov_in_ms = self.nsc_in_ms / 2\n",
    "        self.nov_in_ms = 0\n",
    "        self.nsc_in_sample = int(self.nsc_in_ms / 1000 * self.sr)\n",
    "        self.nov_in_sample = int(self.nov_in_ms / 1000 * self.sr)\n",
    "        \n",
    "        self.key2label = key2label\n",
    "        self.label2key = label2key\n",
    "        \n",
    "    def shuffle_dataset_order(self):\n",
    "        random.shuffle(self.wav_file_name_list)\n",
    "        \n",
    "    def batch_generator(self):\n",
    "        \n",
    "        self.shuffle_dataset_order()\n",
    "        \n",
    "        spectrogram_list = list()\n",
    "        label_list = list()\n",
    "        \n",
    "        for i, wav_file_name in enumerate(self.wav_file_name_list):\n",
    "            \n",
    "            audio = AudioSegment.from_wav(self.dataset_dir + wav_file_name)\n",
    "            \n",
    "            normalized_audio_array = np.asarray(audio.get_array_of_samples()) / 2 ** 15\n",
    "    \n",
    "            f, t, Zxx = scipy_signal.stft(normalized_audio_array, fs=self.sr, \n",
    "                                          nperseg=self.nsc_in_sample,\n",
    "                                          noverlap=self.nov_in_sample)\n",
    "        \n",
    "            Sxx = np.abs(Zxx)\n",
    "            \n",
    "            normalized_spectrogram = (20 * np.log10(np.maximum(Sxx, 1e-8)) + 160) / 160\n",
    "            spectrogram_list.append(normalized_spectrogram)\n",
    "            \n",
    "            keyword = wav_file_name.split('/')[0]\n",
    "            label = self.key2label[keyword]\n",
    "            label_list.append(label)\n",
    "            \n",
    "\n",
    "            if (i + 1) % self.batch_size == 0 or i + 1 == len(self.wav_file_name_list):\n",
    "                \n",
    "                spectrogram_time_step_list = [specgram.shape[1] for specgram in spectrogram_list]\n",
    "                \n",
    "                max_time_step = max(spectrogram_time_step_list)\n",
    "                \n",
    "                freq_size = spectrogram_list[0].shape[0]\n",
    "                \n",
    "                batch = np.zeros([len(spectrogram_list), freq_size, max_time_step])\n",
    "                \n",
    "                for j, specgram in enumerate(spectrogram_list):\n",
    "                    batch[j, :specgram.shape[0], :specgram.shape[1]] = specgram\n",
    "                \n",
    "                spectrogram_list = list()\n",
    "                \n",
    "#                 print('{}:{}'.format(i, batch.shape[0]))\n",
    "            \n",
    "                yield batch, np.asarray(label_list)\n",
    "                \n",
    "                label_list = list()\n",
    "                \n",
    "#                 break\n",
    "    \n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51088"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader_training = DatasetLoader(256, dataset_dir, training_list, key2label_dict, label2key_list)\n",
    "dataset_loader_testing = DatasetLoader(256, dataset_dir, testing_list, key2label_dict, label2key_list)\n",
    "dataset_loader_validation = DatasetLoader(256, dataset_dir, validation_list, key2label_dict, label2key_list)\n",
    "\n",
    "# dataset_loader_testing = DatasetLoader(64, dataset_dir, testing_list, key2label_dict, label2key_list)\n",
    "# dataset_loader_validation = DatasetLoader(64, dataset_dir, validation_list, key2label_dict, label2key_list)\n",
    "\n",
    "# dataset_loader_training = DatasetLoader(768, dataset_dir, training_list, key2label_dict, label2key_list)\n",
    "# dataset_loader_testing = DatasetLoader(768, dataset_dir, testing_list, key2label_dict, label2key_list)\n",
    "# dataset_loader_validation = DatasetLoader(768, dataset_dir, validation_list, key2label_dict, label2key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_generator = dataset_loader_training.batch_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class basic_model(nn.Module):\n",
    "#     def __init__(self, D_in, H, num_layers, num_labels):\n",
    "#         super(basic_model, self).__init__()\n",
    "#         self.fc = torch.nn.Linear(D_in, H)\n",
    "#         self.relu = torch.nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "#         self.gru_layers = nn.ModuleList([nn.GRU(H, int(H / 2), bidirectional=True, batch_first=True) for i in range(num_layers)])\n",
    "\n",
    "#         self.fc_pred = nn.Linear(H, num_labels)\n",
    "#         self.log_softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "#     def forward(self, input_tensor):\n",
    "#         # (B, T, F)\n",
    "#         output_tensor = self.fc(input_tensor)\n",
    "#         output_tensor = self.relu(output_tensor)\n",
    "#         output_tensor = self.dropout(output_tensor)\n",
    "#         # (B, T, H)\n",
    "#         for layer in self.gru_layers:\n",
    "#             output_tensor, _ = layer(output_tensor)\n",
    "            \n",
    "#         output_tensor = self.fc_pred(output_tensor)\n",
    "\n",
    "#         output_tensor = self.log_softmax(output_tensor)\n",
    "        \n",
    "#         return output_tensor\n",
    "\n",
    "class basic_conv_model(nn.Module):\n",
    "#     def __init__(self, first_kernel_size, second_kernel_size, dropout_rate, num_labels):\n",
    "    def __init__(self):\n",
    "        super(basic_conv_model, self).__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(1, 16, (13, 2))\n",
    "        self.conv2d_2 = nn.Conv2d(16, 32, (13, 2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(4800, 31)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        input_tensor.unsqueeze_(1)\n",
    "        tensor = self.conv2d_1(input_tensor)\n",
    "#         print('[After 1st Conv2d]: {}'.format(tensor.shape))\n",
    "        tensor = F.relu(tensor)\n",
    "        tensor = F.dropout(tensor, 0.1, training=True)\n",
    "        tensor = F.max_pool2d(tensor, (3, 2))\n",
    "#         print('[After 1st maxpool2d]: {}'.format(tensor.shape))\n",
    "        tensor = self.conv2d_2(tensor)\n",
    "#         print('[After 2nd Conv2d]: {}'.format(tensor.shape))\n",
    "        tensor = F.relu(tensor)\n",
    "        tensor = F.dropout(tensor, 0.1, training=True)\n",
    "        tensor = F.max_pool2d(tensor, (3, 2))\n",
    "#         print('[After 2nd maxpool2d]: {}'.format(tensor.shape))\n",
    "        tensor = self.flatten(tensor)\n",
    "#         print('[After Flatten]: {}'.format(tensor.shape))\n",
    "        tensor = self.fc(tensor)\n",
    "#         print('[After fc]: {}'.format(tensor.shape))\n",
    "        pred_tensor = F.log_softmax(tensor, dim=-1)\n",
    "        \n",
    "        return pred_tensor\n",
    "    \n",
    "class basic_crnn_model(nn.Module):\n",
    "#     def __init__(self, first_kernel_size, second_kernel_size, dropout_rate, num_labels):\n",
    "    def __init__(self):\n",
    "        super(basic_crnn_model, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(1, 16, (13, 2))\n",
    "        self.gru_1 = nn.GRU(16 * 103, 128, 1)\n",
    "        self.gru_2 = nn.GRU(128, 128, 1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(128 * 12, 31)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        input_tensor.unsqueeze_(1)  # (B, 1, F, T)\n",
    "        tensor = self.conv2d(input_tensor) # (B, 16, F, T)\n",
    "#         print('[After 1st Conv2d]: {}'.format(tensor.shape))\n",
    "        tensor = F.relu(tensor)\n",
    "        tensor = F.dropout(tensor, 0.1, training=True)\n",
    "        tensor = F.max_pool2d(tensor, (3, 2))\n",
    "#         print('[After 1st maxpool2d]: {}'.format(tensor.shape))\n",
    "        \n",
    "        tensor = tensor.view(tensor.shape[0], -1, tensor.shape[3])\n",
    "        # (B, 16 * F, T)\n",
    "        \n",
    "#         print('[After Reshape]: {}'.format(tensor.shape))\n",
    "        \n",
    "        tensor.transpose_(0, 2) # (T, F, B)\n",
    "        tensor.transpose_(1, 2) # (T, B, F)\n",
    "\n",
    "        tensor, _ = self.gru_1(tensor)\n",
    "#         print('[After 2nd Conv2d]: {}'.format(tensor.shape))\n",
    "        tensor = F.relu(tensor)\n",
    "        tensor = F.dropout(tensor, 0.1, training=True) # (T, B, F)\n",
    "#         tensor = F.max_pool2d(tensor, (3, 2))\n",
    "#         print('[After 2nd maxpool2d]: {}'.format(tensor.shape))\n",
    "\n",
    "        tensor, _ = self.gru_2(tensor)\n",
    "#         print('[After 2nd Conv2d]: {}'.format(tensor.shape))\n",
    "        tensor = F.relu(tensor)\n",
    "        tensor = F.dropout(tensor, 0.1, training=True) # (T, B, F)\n",
    "\n",
    "        tensor.transpose_(0, 1) # (B, T, F)\n",
    "    \n",
    "#         print('[After Reshape]: {}'.format(tensor.shape))\n",
    "    \n",
    "        tensor = self.flatten(tensor)\n",
    "#         print('[After Flatten]: {}'.format(tensor.shape))\n",
    "        tensor = self.fc(tensor)\n",
    "#         print('[After fc]: {}'.format(tensor.shape))\n",
    "        pred_tensor = F.log_softmax(tensor, dim=-1)\n",
    "        \n",
    "        return pred_tensor\n",
    "\n",
    "    \n",
    "# model = basic_model(321, 512, 3, 31).float().to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# dictation_loss = nn.CTCLoss().to(device)\n",
    "\n",
    "model = basic_crnn_model().float().to(device)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_generator = dataset_loader_training.batch_generator()\n",
    "# next(batch_generator)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:56:18.907031\n",
      "#000 [Train: 3.373] [Test: 3.214] [Valid: 3.195]\n",
      "#000 [Train: 0.052] [Test: 0.097] [Valid: 0.102]\n",
      "09:57:01.858595\n",
      "#001 [Train: 2.995] [Test: 2.857] [Valid: 2.811]\n",
      "#001 [Train: 0.133] [Test: 0.161] [Valid: 0.170]\n",
      "09:57:44.780909\n",
      "#002 [Train: 2.722] [Test: 2.626] [Valid: 2.570]\n",
      "#002 [Train: 0.194] [Test: 0.230] [Valid: 0.242]\n",
      "09:58:27.698783\n",
      "#003 [Train: 2.482] [Test: 2.394] [Valid: 2.339]\n",
      "#003 [Train: 0.270] [Test: 0.300] [Valid: 0.312]\n",
      "09:59:10.729282\n",
      "#004 [Train: 2.279] [Test: 2.215] [Valid: 2.169]\n",
      "#004 [Train: 0.329] [Test: 0.355] [Valid: 0.365]\n",
      "09:59:53.807721\n",
      "#005 [Train: 2.105] [Test: 2.052] [Valid: 2.010]\n",
      "#005 [Train: 0.383] [Test: 0.405] [Valid: 0.418]\n",
      "10:00:36.918494\n",
      "#006 [Train: 1.921] [Test: 1.878] [Valid: 1.827]\n",
      "#006 [Train: 0.438] [Test: 0.453] [Valid: 0.461]\n",
      "10:01:20.052360\n",
      "#007 [Train: 1.742] [Test: 1.714] [Valid: 1.673]\n",
      "#007 [Train: 0.495] [Test: 0.500] [Valid: 0.521]\n",
      "10:02:03.083466\n",
      "#008 [Train: 1.597] [Test: 1.596] [Valid: 1.557]\n",
      "#008 [Train: 0.537] [Test: 0.543] [Valid: 0.548]\n",
      "10:02:46.111733\n",
      "#009 [Train: 1.475] [Test: 1.504] [Valid: 1.447]\n",
      "#009 [Train: 0.572] [Test: 0.566] [Valid: 0.579]\n",
      "10:03:29.221429\n",
      "#010 [Train: 1.370] [Test: 1.407] [Valid: 1.359]\n",
      "#010 [Train: 0.604] [Test: 0.594] [Valid: 0.603]\n",
      "10:04:12.254900\n",
      "#011 [Train: 1.281] [Test: 1.325] [Valid: 1.280]\n",
      "#011 [Train: 0.628] [Test: 0.617] [Valid: 0.624]\n",
      "10:04:55.306739\n",
      "#012 [Train: 1.204] [Test: 1.234] [Valid: 1.196]\n",
      "#012 [Train: 0.650] [Test: 0.641] [Valid: 0.653]\n",
      "10:05:38.377363\n",
      "#013 [Train: 1.137] [Test: 1.169] [Valid: 1.137]\n",
      "#013 [Train: 0.670] [Test: 0.661] [Valid: 0.667]\n",
      "10:06:21.425477\n",
      "#014 [Train: 1.078] [Test: 1.110] [Valid: 1.090]\n",
      "#014 [Train: 0.687] [Test: 0.675] [Valid: 0.682]\n",
      "10:07:04.417481\n",
      "#015 [Train: 1.029] [Test: 1.093] [Valid: 1.045]\n",
      "#015 [Train: 0.699] [Test: 0.679] [Valid: 0.692]\n",
      "10:07:47.341130\n",
      "#016 [Train: 0.982] [Test: 1.034] [Valid: 0.998]\n",
      "#016 [Train: 0.713] [Test: 0.698] [Valid: 0.704]\n",
      "10:08:30.345802\n",
      "#017 [Train: 0.942] [Test: 0.995] [Valid: 0.958]\n",
      "#017 [Train: 0.725] [Test: 0.709] [Valid: 0.718]\n",
      "10:09:13.284753\n",
      "#018 [Train: 0.905] [Test: 0.949] [Valid: 0.920]\n",
      "#018 [Train: 0.737] [Test: 0.722] [Valid: 0.729]\n",
      "10:09:56.182099\n",
      "#019 [Train: 0.870] [Test: 0.930] [Valid: 0.903]\n",
      "#019 [Train: 0.746] [Test: 0.727] [Valid: 0.731]\n",
      "10:10:39.267668\n",
      "#020 [Train: 0.840] [Test: 0.911] [Valid: 0.895]\n",
      "#020 [Train: 0.755] [Test: 0.735] [Valid: 0.735]\n",
      "10:11:22.138194\n",
      "#021 [Train: 0.806] [Test: 0.861] [Valid: 0.849]\n",
      "#021 [Train: 0.766] [Test: 0.747] [Valid: 0.746]\n",
      "10:12:05.054914\n",
      "#022 [Train: 0.778] [Test: 0.851] [Valid: 0.823]\n",
      "#022 [Train: 0.773] [Test: 0.750] [Valid: 0.754]\n",
      "10:12:47.957599\n",
      "#023 [Train: 0.753] [Test: 0.823] [Valid: 0.808]\n",
      "#023 [Train: 0.780] [Test: 0.756] [Valid: 0.758]\n",
      "10:13:31.129900\n",
      "#024 [Train: 0.730] [Test: 0.791] [Valid: 0.776]\n",
      "#024 [Train: 0.787] [Test: 0.766] [Valid: 0.770]\n",
      "10:14:13.985975\n",
      "#025 [Train: 0.707] [Test: 0.796] [Valid: 0.765]\n",
      "#025 [Train: 0.793] [Test: 0.762] [Valid: 0.773]\n",
      "10:14:56.992130\n",
      "#026 [Train: 0.689] [Test: 0.761] [Valid: 0.744]\n",
      "#026 [Train: 0.799] [Test: 0.775] [Valid: 0.779]\n",
      "10:15:39.955199\n",
      "#027 [Train: 0.671] [Test: 0.744] [Valid: 0.742]\n",
      "#027 [Train: 0.802] [Test: 0.778] [Valid: 0.779]\n",
      "10:16:22.860927\n",
      "#028 [Train: 0.650] [Test: 0.725] [Valid: 0.714]\n",
      "#028 [Train: 0.809] [Test: 0.786] [Valid: 0.791]\n",
      "10:17:05.780071\n",
      "#029 [Train: 0.633] [Test: 0.708] [Valid: 0.705]\n",
      "#029 [Train: 0.815] [Test: 0.785] [Valid: 0.793]\n",
      "10:17:48.679867\n",
      "#030 [Train: 0.616] [Test: 0.692] [Valid: 0.680]\n",
      "#030 [Train: 0.819] [Test: 0.796] [Valid: 0.799]\n",
      "10:18:31.713419\n",
      "#031 [Train: 0.598] [Test: 0.711] [Valid: 0.713]\n",
      "#031 [Train: 0.825] [Test: 0.790] [Valid: 0.789]\n",
      "10:19:14.757607\n",
      "#032 [Train: 0.585] [Test: 0.696] [Valid: 0.688]\n",
      "#032 [Train: 0.828] [Test: 0.789] [Valid: 0.795]\n",
      "10:19:57.807822\n",
      "#033 [Train: 0.571] [Test: 0.666] [Valid: 0.646]\n",
      "#033 [Train: 0.832] [Test: 0.805] [Valid: 0.813]\n",
      "10:20:40.810342\n",
      "#034 [Train: 0.558] [Test: 0.672] [Valid: 0.655]\n",
      "#034 [Train: 0.837] [Test: 0.798] [Valid: 0.801]\n",
      "10:21:23.838424\n",
      "#035 [Train: 0.545] [Test: 0.642] [Valid: 0.631]\n",
      "#035 [Train: 0.840] [Test: 0.808] [Valid: 0.810]\n",
      "10:22:06.931319\n",
      "#036 [Train: 0.532] [Test: 0.610] [Valid: 0.611]\n",
      "#036 [Train: 0.844] [Test: 0.822] [Valid: 0.822]\n",
      "10:22:49.858132\n",
      "#037 [Train: 0.523] [Test: 0.615] [Valid: 0.613]\n",
      "#037 [Train: 0.844] [Test: 0.818] [Valid: 0.817]\n",
      "10:23:32.785426\n",
      "#038 [Train: 0.506] [Test: 0.603] [Valid: 0.599]\n",
      "#038 [Train: 0.852] [Test: 0.821] [Valid: 0.818]\n",
      "10:24:15.682934\n",
      "#039 [Train: 0.501] [Test: 0.593] [Valid: 0.583]\n",
      "#039 [Train: 0.854] [Test: 0.822] [Valid: 0.827]\n",
      "10:24:58.687322\n",
      "#040 [Train: 0.490] [Test: 0.579] [Valid: 0.581]\n",
      "#040 [Train: 0.855] [Test: 0.825] [Valid: 0.827]\n",
      "10:25:41.590353\n",
      "#041 [Train: 0.482] [Test: 0.606] [Valid: 0.601]\n",
      "#041 [Train: 0.857] [Test: 0.818] [Valid: 0.823]\n",
      "10:26:24.565238\n",
      "#042 [Train: 0.469] [Test: 0.574] [Valid: 0.568]\n",
      "#042 [Train: 0.862] [Test: 0.829] [Valid: 0.830]\n",
      "10:27:07.722603\n",
      "#043 [Train: 0.457] [Test: 0.572] [Valid: 0.567]\n",
      "#043 [Train: 0.866] [Test: 0.828] [Valid: 0.831]\n",
      "10:27:50.804803\n",
      "#044 [Train: 0.448] [Test: 0.556] [Valid: 0.553]\n",
      "#044 [Train: 0.867] [Test: 0.833] [Valid: 0.833]\n",
      "10:28:33.919710\n",
      "#045 [Train: 0.443] [Test: 0.545] [Valid: 0.552]\n",
      "#045 [Train: 0.871] [Test: 0.836] [Valid: 0.837]\n",
      "10:29:16.841447\n",
      "#046 [Train: 0.432] [Test: 0.551] [Valid: 0.539]\n",
      "#046 [Train: 0.873] [Test: 0.835] [Valid: 0.841]\n",
      "10:29:59.743894\n",
      "#047 [Train: 0.431] [Test: 0.543] [Valid: 0.540]\n",
      "#047 [Train: 0.874] [Test: 0.841] [Valid: 0.841]\n",
      "10:30:42.611153\n",
      "#048 [Train: 0.423] [Test: 0.532] [Valid: 0.522]\n",
      "#048 [Train: 0.875] [Test: 0.846] [Valid: 0.845]\n",
      "10:31:25.648780\n",
      "#049 [Train: 0.414] [Test: 0.551] [Valid: 0.559]\n",
      "#049 [Train: 0.877] [Test: 0.836] [Valid: 0.832]\n",
      "10:32:08.546750\n",
      "#050 [Train: 0.409] [Test: 0.521] [Valid: 0.510]\n",
      "#050 [Train: 0.880] [Test: 0.847] [Valid: 0.846]\n",
      "10:32:51.507429\n",
      "#051 [Train: 0.399] [Test: 0.512] [Valid: 0.515]\n",
      "#051 [Train: 0.881] [Test: 0.849] [Valid: 0.845]\n",
      "10:33:34.409073\n",
      "#052 [Train: 0.391] [Test: 0.512] [Valid: 0.511]\n",
      "#052 [Train: 0.885] [Test: 0.849] [Valid: 0.849]\n",
      "10:34:17.457583\n",
      "#053 [Train: 0.388] [Test: 0.524] [Valid: 0.520]\n",
      "#053 [Train: 0.886] [Test: 0.843] [Valid: 0.844]\n",
      "10:35:00.508197\n",
      "#054 [Train: 0.382] [Test: 0.513] [Valid: 0.516]\n",
      "#054 [Train: 0.887] [Test: 0.847] [Valid: 0.844]\n",
      "10:35:43.537194\n",
      "#055 [Train: 0.374] [Test: 0.509] [Valid: 0.516]\n",
      "#055 [Train: 0.890] [Test: 0.846] [Valid: 0.847]\n",
      "10:36:26.550563\n",
      "#056 [Train: 0.371] [Test: 0.513] [Valid: 0.504]\n",
      "#056 [Train: 0.890] [Test: 0.847] [Valid: 0.854]\n",
      "10:37:09.544471\n",
      "#057 [Train: 0.364] [Test: 0.477] [Valid: 0.478]\n",
      "#057 [Train: 0.893] [Test: 0.856] [Valid: 0.855]\n",
      "10:37:52.560683\n",
      "#058 [Train: 0.359] [Test: 0.487] [Valid: 0.488]\n",
      "#058 [Train: 0.895] [Test: 0.854] [Valid: 0.854]\n",
      "10:38:35.591320\n",
      "#059 [Train: 0.356] [Test: 0.487] [Valid: 0.472]\n",
      "#059 [Train: 0.895] [Test: 0.856] [Valid: 0.860]\n",
      "10:39:18.752382\n",
      "#060 [Train: 0.347] [Test: 0.456] [Valid: 0.468]\n",
      "#060 [Train: 0.898] [Test: 0.867] [Valid: 0.861]\n",
      "10:40:01.899190\n",
      "#061 [Train: 0.341] [Test: 0.475] [Valid: 0.477]\n",
      "#061 [Train: 0.899] [Test: 0.854] [Valid: 0.860]\n",
      "10:40:44.974790\n",
      "#062 [Train: 0.337] [Test: 0.468] [Valid: 0.471]\n",
      "#062 [Train: 0.900] [Test: 0.862] [Valid: 0.859]\n",
      "10:41:28.068903\n",
      "#063 [Train: 0.333] [Test: 0.467] [Valid: 0.460]\n",
      "#063 [Train: 0.902] [Test: 0.860] [Valid: 0.862]\n",
      "10:42:11.125283\n",
      "#064 [Train: 0.330] [Test: 0.459] [Valid: 0.455]\n",
      "#064 [Train: 0.904] [Test: 0.860] [Valid: 0.867]\n",
      "10:42:54.164945\n",
      "#065 [Train: 0.322] [Test: 0.474] [Valid: 0.463]\n",
      "#065 [Train: 0.905] [Test: 0.862] [Valid: 0.865]\n",
      "10:43:37.250876\n",
      "#066 [Train: 0.319] [Test: 0.482] [Valid: 0.466]\n",
      "#066 [Train: 0.906] [Test: 0.861] [Valid: 0.865]\n",
      "10:44:20.176357\n",
      "#067 [Train: 0.313] [Test: 0.441] [Valid: 0.444]\n",
      "#067 [Train: 0.909] [Test: 0.870] [Valid: 0.869]\n",
      "10:45:03.192580\n",
      "#068 [Train: 0.309] [Test: 0.457] [Valid: 0.460]\n",
      "#068 [Train: 0.908] [Test: 0.861] [Valid: 0.868]\n",
      "10:45:46.260014\n",
      "#069 [Train: 0.308] [Test: 0.448] [Valid: 0.447]\n",
      "#069 [Train: 0.909] [Test: 0.868] [Valid: 0.870]\n",
      "10:46:29.334175\n",
      "#070 [Train: 0.306] [Test: 0.440] [Valid: 0.443]\n",
      "#070 [Train: 0.910] [Test: 0.866] [Valid: 0.867]\n",
      "10:47:12.404513\n",
      "#071 [Train: 0.296] [Test: 0.460] [Valid: 0.443]\n",
      "#071 [Train: 0.913] [Test: 0.863] [Valid: 0.873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:47:55.435172\n",
      "#072 [Train: 0.295] [Test: 0.424] [Valid: 0.431]\n",
      "#072 [Train: 0.913] [Test: 0.872] [Valid: 0.873]\n",
      "10:48:38.346640\n",
      "#073 [Train: 0.290] [Test: 0.439] [Valid: 0.435]\n",
      "#073 [Train: 0.914] [Test: 0.870] [Valid: 0.870]\n",
      "10:49:21.257843\n",
      "#074 [Train: 0.286] [Test: 0.428] [Valid: 0.423]\n",
      "#074 [Train: 0.917] [Test: 0.874] [Valid: 0.875]\n",
      "10:50:04.306388\n",
      "#075 [Train: 0.280] [Test: 0.431] [Valid: 0.428]\n",
      "#075 [Train: 0.918] [Test: 0.874] [Valid: 0.873]\n",
      "10:50:47.338657\n",
      "#076 [Train: 0.280] [Test: 0.434] [Valid: 0.435]\n",
      "#076 [Train: 0.917] [Test: 0.870] [Valid: 0.873]\n",
      "10:51:30.339826\n",
      "#077 [Train: 0.274] [Test: 0.419] [Valid: 0.423]\n",
      "#077 [Train: 0.920] [Test: 0.876] [Valid: 0.877]\n",
      "10:52:13.315375\n",
      "#078 [Train: 0.269] [Test: 0.425] [Valid: 0.441]\n",
      "#078 [Train: 0.921] [Test: 0.873] [Valid: 0.873]\n",
      "10:52:56.353072\n",
      "#079 [Train: 0.270] [Test: 0.420] [Valid: 0.425]\n",
      "#079 [Train: 0.920] [Test: 0.877] [Valid: 0.875]\n",
      "10:53:39.580271\n",
      "#080 [Train: 0.266] [Test: 0.411] [Valid: 0.410]\n",
      "#080 [Train: 0.922] [Test: 0.874] [Valid: 0.877]\n",
      "10:54:22.798264\n",
      "#081 [Train: 0.260] [Test: 0.435] [Valid: 0.427]\n",
      "#081 [Train: 0.923] [Test: 0.874] [Valid: 0.876]\n",
      "10:55:05.749558\n",
      "#082 [Train: 0.257] [Test: 0.415] [Valid: 0.414]\n",
      "#082 [Train: 0.925] [Test: 0.875] [Valid: 0.881]\n",
      "10:55:48.703021\n",
      "#083 [Train: 0.253] [Test: 0.412] [Valid: 0.414]\n",
      "#083 [Train: 0.926] [Test: 0.878] [Valid: 0.878]\n",
      "10:56:31.681430\n",
      "#084 [Train: 0.252] [Test: 0.440] [Valid: 0.436]\n",
      "#084 [Train: 0.926] [Test: 0.873] [Valid: 0.872]\n",
      "10:57:14.607107\n",
      "#085 [Train: 0.251] [Test: 0.406] [Valid: 0.422]\n",
      "#085 [Train: 0.927] [Test: 0.878] [Valid: 0.879]\n",
      "10:57:57.563653\n",
      "#086 [Train: 0.250] [Test: 0.409] [Valid: 0.405]\n",
      "#086 [Train: 0.927] [Test: 0.881] [Valid: 0.882]\n",
      "10:58:40.544860\n",
      "#087 [Train: 0.239] [Test: 0.405] [Valid: 0.404]\n",
      "#087 [Train: 0.930] [Test: 0.883] [Valid: 0.879]\n",
      "10:59:23.487949\n",
      "#088 [Train: 0.239] [Test: 0.402] [Valid: 0.408]\n",
      "#088 [Train: 0.930] [Test: 0.880] [Valid: 0.882]\n",
      "11:00:06.465028\n",
      "#089 [Train: 0.238] [Test: 0.406] [Valid: 0.413]\n",
      "#089 [Train: 0.930] [Test: 0.881] [Valid: 0.879]\n",
      "11:00:49.504075\n",
      "#090 [Train: 0.233] [Test: 0.397] [Valid: 0.415]\n",
      "#090 [Train: 0.931] [Test: 0.886] [Valid: 0.882]\n",
      "11:01:32.475227\n",
      "#091 [Train: 0.230] [Test: 0.396] [Valid: 0.413]\n",
      "#091 [Train: 0.932] [Test: 0.885] [Valid: 0.882]\n",
      "11:02:15.502211\n",
      "#092 [Train: 0.227] [Test: 0.393] [Valid: 0.396]\n",
      "#092 [Train: 0.933] [Test: 0.884] [Valid: 0.881]\n",
      "11:02:58.475078\n",
      "#093 [Train: 0.228] [Test: 0.389] [Valid: 0.393]\n",
      "#093 [Train: 0.934] [Test: 0.885] [Valid: 0.887]\n",
      "11:03:41.426503\n",
      "#094 [Train: 0.220] [Test: 0.411] [Valid: 0.420]\n",
      "#094 [Train: 0.935] [Test: 0.881] [Valid: 0.880]\n",
      "11:04:24.290209\n",
      "#095 [Train: 0.219] [Test: 0.397] [Valid: 0.399]\n",
      "#095 [Train: 0.937] [Test: 0.882] [Valid: 0.882]\n",
      "11:05:07.205274\n",
      "#096 [Train: 0.218] [Test: 0.395] [Valid: 0.394]\n",
      "#096 [Train: 0.936] [Test: 0.888] [Valid: 0.887]\n",
      "11:05:50.138367\n",
      "#097 [Train: 0.217] [Test: 0.414] [Valid: 0.413]\n",
      "#097 [Train: 0.938] [Test: 0.878] [Valid: 0.879]\n",
      "11:06:33.142516\n",
      "#098 [Train: 0.215] [Test: 0.402] [Valid: 0.410]\n",
      "#098 [Train: 0.938] [Test: 0.885] [Valid: 0.883]\n",
      "11:07:16.086858\n",
      "#099 [Train: 0.211] [Test: 0.386] [Valid: 0.396]\n",
      "#099 [Train: 0.938] [Test: 0.888] [Valid: 0.885]\n",
      "11:07:59.019454\n",
      "#100 [Train: 0.207] [Test: 0.406] [Valid: 0.409]\n",
      "#100 [Train: 0.939] [Test: 0.885] [Valid: 0.884]\n",
      "11:08:41.913622\n",
      "#101 [Train: 0.206] [Test: 0.391] [Valid: 0.388]\n",
      "#101 [Train: 0.940] [Test: 0.888] [Valid: 0.890]\n",
      "11:09:24.787791\n",
      "#102 [Train: 0.201] [Test: 0.382] [Valid: 0.383]\n",
      "#102 [Train: 0.941] [Test: 0.889] [Valid: 0.889]\n",
      "11:10:07.600229\n",
      "#103 [Train: 0.200] [Test: 0.394] [Valid: 0.397]\n",
      "#103 [Train: 0.942] [Test: 0.885] [Valid: 0.886]\n",
      "11:10:50.425074\n",
      "#104 [Train: 0.198] [Test: 0.403] [Valid: 0.407]\n",
      "#104 [Train: 0.942] [Test: 0.884] [Valid: 0.883]\n",
      "11:11:33.369088\n",
      "#105 [Train: 0.197] [Test: 0.371] [Valid: 0.384]\n",
      "#105 [Train: 0.941] [Test: 0.893] [Valid: 0.886]\n",
      "11:12:16.144275\n",
      "#106 [Train: 0.195] [Test: 0.394] [Valid: 0.399]\n",
      "#106 [Train: 0.943] [Test: 0.885] [Valid: 0.888]\n",
      "11:12:58.871056\n",
      "#107 [Train: 0.191] [Test: 0.387] [Valid: 0.392]\n",
      "#107 [Train: 0.945] [Test: 0.888] [Valid: 0.886]\n",
      "11:13:41.488288\n",
      "#108 [Train: 0.190] [Test: 0.374] [Valid: 0.382]\n",
      "#108 [Train: 0.945] [Test: 0.892] [Valid: 0.893]\n",
      "11:14:24.003062\n",
      "#109 [Train: 0.186] [Test: 0.384] [Valid: 0.404]\n",
      "#109 [Train: 0.946] [Test: 0.891] [Valid: 0.886]\n",
      "11:15:06.571272\n",
      "#110 [Train: 0.184] [Test: 0.388] [Valid: 0.387]\n",
      "#110 [Train: 0.947] [Test: 0.888] [Valid: 0.889]\n",
      "11:15:49.221933\n",
      "#111 [Train: 0.183] [Test: 0.381] [Valid: 0.395]\n",
      "#111 [Train: 0.946] [Test: 0.892] [Valid: 0.887]\n",
      "11:16:31.895355\n",
      "#112 [Train: 0.181] [Test: 0.399] [Valid: 0.405]\n",
      "#112 [Train: 0.947] [Test: 0.889] [Valid: 0.885]\n",
      "11:17:14.571448\n",
      "#113 [Train: 0.179] [Test: 0.387] [Valid: 0.389]\n",
      "#113 [Train: 0.949] [Test: 0.893] [Valid: 0.893]\n",
      "11:17:57.225934\n",
      "#114 [Train: 0.175] [Test: 0.392] [Valid: 0.395]\n",
      "#114 [Train: 0.949] [Test: 0.889] [Valid: 0.888]\n",
      "11:18:39.886394\n",
      "#115 [Train: 0.172] [Test: 0.382] [Valid: 0.389]\n",
      "#115 [Train: 0.950] [Test: 0.892] [Valid: 0.887]\n",
      "11:19:22.518291\n",
      "#116 [Train: 0.174] [Test: 0.382] [Valid: 0.396]\n",
      "#116 [Train: 0.949] [Test: 0.894] [Valid: 0.888]\n",
      "11:20:05.144063\n",
      "#117 [Train: 0.171] [Test: 0.377] [Valid: 0.375]\n",
      "#117 [Train: 0.951] [Test: 0.891] [Valid: 0.896]\n",
      "11:20:47.674807\n",
      "#118 [Train: 0.169] [Test: 0.379] [Valid: 0.377]\n",
      "#118 [Train: 0.951] [Test: 0.893] [Valid: 0.892]\n",
      "11:21:30.226184\n",
      "#119 [Train: 0.166] [Test: 0.399] [Valid: 0.405]\n",
      "#119 [Train: 0.951] [Test: 0.886] [Valid: 0.887]\n",
      "11:22:12.786146\n",
      "#120 [Train: 0.166] [Test: 0.379] [Valid: 0.379]\n",
      "#120 [Train: 0.952] [Test: 0.892] [Valid: 0.894]\n",
      "11:22:55.277201\n",
      "#121 [Train: 0.163] [Test: 0.364] [Valid: 0.378]\n",
      "#121 [Train: 0.952] [Test: 0.896] [Valid: 0.890]\n",
      "11:23:37.799949\n",
      "#122 [Train: 0.161] [Test: 0.367] [Valid: 0.380]\n",
      "#122 [Train: 0.953] [Test: 0.895] [Valid: 0.892]\n",
      "11:24:20.357067\n",
      "#123 [Train: 0.159] [Test: 0.373] [Valid: 0.376]\n",
      "#123 [Train: 0.954] [Test: 0.895] [Valid: 0.893]\n",
      "11:25:02.849042\n",
      "#124 [Train: 0.158] [Test: 0.374] [Valid: 0.387]\n",
      "#124 [Train: 0.954] [Test: 0.895] [Valid: 0.893]\n",
      "11:25:45.387255\n",
      "#125 [Train: 0.156] [Test: 0.379] [Valid: 0.379]\n",
      "#125 [Train: 0.955] [Test: 0.896] [Valid: 0.893]\n",
      "11:26:27.890391\n",
      "#126 [Train: 0.155] [Test: 0.364] [Valid: 0.384]\n",
      "#126 [Train: 0.956] [Test: 0.896] [Valid: 0.888]\n",
      "11:27:10.387507\n",
      "#127 [Train: 0.151] [Test: 0.380] [Valid: 0.382]\n",
      "#127 [Train: 0.955] [Test: 0.894] [Valid: 0.892]\n",
      "11:27:52.850736\n",
      "#128 [Train: 0.149] [Test: 0.374] [Valid: 0.389]\n",
      "#128 [Train: 0.957] [Test: 0.896] [Valid: 0.893]\n",
      "11:28:35.404004\n",
      "#129 [Train: 0.150] [Test: 0.371] [Valid: 0.380]\n",
      "#129 [Train: 0.957] [Test: 0.894] [Valid: 0.894]\n",
      "11:29:17.839639\n",
      "#130 [Train: 0.149] [Test: 0.390] [Valid: 0.384]\n",
      "#130 [Train: 0.957] [Test: 0.892] [Valid: 0.893]\n",
      "11:30:00.308603\n",
      "#131 [Train: 0.147] [Test: 0.375] [Valid: 0.383]\n",
      "#131 [Train: 0.958] [Test: 0.892] [Valid: 0.896]\n",
      "11:30:42.832874\n",
      "#132 [Train: 0.144] [Test: 0.379] [Valid: 0.390]\n",
      "#132 [Train: 0.957] [Test: 0.895] [Valid: 0.892]\n",
      "11:31:25.379532\n",
      "#133 [Train: 0.140] [Test: 0.378] [Valid: 0.393]\n",
      "#133 [Train: 0.959] [Test: 0.896] [Valid: 0.893]\n",
      "11:32:07.882830\n",
      "#134 [Train: 0.143] [Test: 0.375] [Valid: 0.388]\n",
      "#134 [Train: 0.959] [Test: 0.897] [Valid: 0.895]\n",
      "11:32:50.347986\n",
      "#135 [Train: 0.140] [Test: 0.383] [Valid: 0.391]\n",
      "#135 [Train: 0.960] [Test: 0.892] [Valid: 0.896]\n",
      "11:33:32.863208\n",
      "#136 [Train: 0.138] [Test: 0.367] [Valid: 0.377]\n",
      "#136 [Train: 0.960] [Test: 0.897] [Valid: 0.894]\n",
      "11:34:15.387934\n",
      "#137 [Train: 0.137] [Test: 0.367] [Valid: 0.387]\n",
      "#137 [Train: 0.961] [Test: 0.898] [Valid: 0.896]\n",
      "11:34:58.014687\n",
      "#138 [Train: 0.137] [Test: 0.386] [Valid: 0.383]\n",
      "#138 [Train: 0.960] [Test: 0.895] [Valid: 0.894]\n",
      "11:35:40.649623\n",
      "#139 [Train: 0.132] [Test: 0.394] [Valid: 0.398]\n",
      "#139 [Train: 0.963] [Test: 0.892] [Valid: 0.893]\n",
      "11:36:23.280327\n",
      "#140 [Train: 0.130] [Test: 0.371] [Valid: 0.383]\n",
      "#140 [Train: 0.962] [Test: 0.896] [Valid: 0.898]\n",
      "11:37:05.905320\n",
      "#141 [Train: 0.131] [Test: 0.371] [Valid: 0.372]\n",
      "#141 [Train: 0.962] [Test: 0.896] [Valid: 0.897]\n",
      "11:37:48.535910\n",
      "#142 [Train: 0.127] [Test: 0.373] [Valid: 0.385]\n",
      "#142 [Train: 0.964] [Test: 0.898] [Valid: 0.894]\n",
      "11:38:31.163314\n",
      "#143 [Train: 0.128] [Test: 0.371] [Valid: 0.382]\n",
      "#143 [Train: 0.964] [Test: 0.899] [Valid: 0.895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:39:13.781862\n",
      "#144 [Train: 0.129] [Test: 0.386] [Valid: 0.394]\n",
      "#144 [Train: 0.963] [Test: 0.893] [Valid: 0.896]\n",
      "11:39:56.415461\n",
      "#145 [Train: 0.126] [Test: 0.369] [Valid: 0.387]\n",
      "#145 [Train: 0.965] [Test: 0.897] [Valid: 0.898]\n",
      "11:40:39.041871\n",
      "#146 [Train: 0.124] [Test: 0.377] [Valid: 0.384]\n",
      "#146 [Train: 0.965] [Test: 0.894] [Valid: 0.894]\n",
      "11:41:21.677739\n",
      "#147 [Train: 0.123] [Test: 0.376] [Valid: 0.380]\n",
      "#147 [Train: 0.964] [Test: 0.897] [Valid: 0.897]\n",
      "11:42:04.275496\n",
      "#148 [Train: 0.122] [Test: 0.390] [Valid: 0.389]\n",
      "#148 [Train: 0.966] [Test: 0.895] [Valid: 0.896]\n",
      "11:42:46.901326\n",
      "#149 [Train: 0.120] [Test: 0.382] [Valid: 0.391]\n",
      "#149 [Train: 0.965] [Test: 0.898] [Valid: 0.892]\n",
      "11:43:29.483853\n",
      "#150 [Train: 0.118] [Test: 0.375] [Valid: 0.383]\n",
      "#150 [Train: 0.967] [Test: 0.897] [Valid: 0.895]\n",
      "11:44:12.085192\n",
      "#151 [Train: 0.118] [Test: 0.384] [Valid: 0.381]\n",
      "#151 [Train: 0.966] [Test: 0.898] [Valid: 0.895]\n",
      "11:44:54.683724\n",
      "#152 [Train: 0.116] [Test: 0.384] [Valid: 0.401]\n",
      "#152 [Train: 0.967] [Test: 0.896] [Valid: 0.895]\n",
      "11:45:37.239460\n",
      "#153 [Train: 0.114] [Test: 0.381] [Valid: 0.392]\n",
      "#153 [Train: 0.969] [Test: 0.896] [Valid: 0.897]\n",
      "11:46:19.813653\n",
      "#154 [Train: 0.115] [Test: 0.376] [Valid: 0.392]\n",
      "#154 [Train: 0.967] [Test: 0.899] [Valid: 0.898]\n",
      "11:47:02.455875\n",
      "#155 [Train: 0.111] [Test: 0.391] [Valid: 0.385]\n",
      "#155 [Train: 0.968] [Test: 0.895] [Valid: 0.898]\n",
      "11:47:44.972086\n",
      "#156 [Train: 0.112] [Test: 0.375] [Valid: 0.390]\n",
      "#156 [Train: 0.969] [Test: 0.898] [Valid: 0.897]\n",
      "11:48:27.644006\n",
      "#157 [Train: 0.109] [Test: 0.388] [Valid: 0.390]\n",
      "#157 [Train: 0.969] [Test: 0.896] [Valid: 0.899]\n",
      "11:49:10.311725\n",
      "#158 [Train: 0.109] [Test: 0.390] [Valid: 0.388]\n",
      "#158 [Train: 0.969] [Test: 0.893] [Valid: 0.900]\n",
      "11:49:52.980118\n",
      "#159 [Train: 0.106] [Test: 0.393] [Valid: 0.401]\n",
      "#159 [Train: 0.970] [Test: 0.896] [Valid: 0.896]\n",
      "11:50:35.486050\n",
      "#160 [Train: 0.108] [Test: 0.390] [Valid: 0.385]\n",
      "#160 [Train: 0.970] [Test: 0.897] [Valid: 0.900]\n",
      "11:51:18.131830\n",
      "#161 [Train: 0.106] [Test: 0.385] [Valid: 0.416]\n",
      "#161 [Train: 0.970] [Test: 0.895] [Valid: 0.893]\n",
      "11:52:00.586158\n",
      "#162 [Train: 0.105] [Test: 0.370] [Valid: 0.382]\n",
      "#162 [Train: 0.969] [Test: 0.902] [Valid: 0.900]\n",
      "11:52:43.022039\n",
      "#163 [Train: 0.103] [Test: 0.385] [Valid: 0.389]\n",
      "#163 [Train: 0.971] [Test: 0.899] [Valid: 0.897]\n",
      "11:53:27.717587\n",
      "#164 [Train: 0.101] [Test: 0.382] [Valid: 0.387]\n",
      "#164 [Train: 0.971] [Test: 0.900] [Valid: 0.899]\n",
      "11:54:14.628534\n",
      "#165 [Train: 0.100] [Test: 0.380] [Valid: 0.380]\n",
      "#165 [Train: 0.972] [Test: 0.900] [Valid: 0.901]\n",
      "11:55:01.016394\n",
      "#166 [Train: 0.101] [Test: 0.387] [Valid: 0.403]\n",
      "#166 [Train: 0.971] [Test: 0.901] [Valid: 0.897]\n",
      "11:55:47.590999\n",
      "#167 [Train: 0.098] [Test: 0.371] [Valid: 0.378]\n",
      "#167 [Train: 0.973] [Test: 0.900] [Valid: 0.903]\n",
      "11:56:34.259285\n",
      "#168 [Train: 0.099] [Test: 0.394] [Valid: 0.388]\n",
      "#168 [Train: 0.972] [Test: 0.900] [Valid: 0.898]\n",
      "11:57:20.954338\n",
      "#169 [Train: 0.096] [Test: 0.389] [Valid: 0.407]\n",
      "#169 [Train: 0.974] [Test: 0.902] [Valid: 0.895]\n",
      "11:58:06.115028\n",
      "#170 [Train: 0.096] [Test: 0.375] [Valid: 0.390]\n",
      "#170 [Train: 0.973] [Test: 0.903] [Valid: 0.899]\n",
      "11:58:48.650229\n",
      "#171 [Train: 0.094] [Test: 0.401] [Valid: 0.408]\n",
      "#171 [Train: 0.973] [Test: 0.898] [Valid: 0.897]\n",
      "11:59:31.151282\n",
      "#172 [Train: 0.094] [Test: 0.404] [Valid: 0.393]\n",
      "#172 [Train: 0.974] [Test: 0.894] [Valid: 0.896]\n",
      "12:00:13.820181\n",
      "#173 [Train: 0.094] [Test: 0.402] [Valid: 0.409]\n",
      "#173 [Train: 0.973] [Test: 0.898] [Valid: 0.897]\n",
      "12:00:56.383136\n",
      "#174 [Train: 0.093] [Test: 0.389] [Valid: 0.392]\n",
      "#174 [Train: 0.974] [Test: 0.901] [Valid: 0.897]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-262cc7cd2ad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mtotal_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;31m#     optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-3350f5aa4304>\u001b[0m in \u001b[0;36mbatch_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m             f, t, Zxx = scipy_signal.stft(normalized_audio_array, fs=self.sr, \n\u001b[1;32m     36\u001b[0m                                           \u001b[0mnperseg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnsc_in_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                                           noverlap=self.nov_in_sample)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mSxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZxx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/avalanche/lib/python3.7/site-packages/scipy/signal/spectral.py\u001b[0m in \u001b[0;36mstft\u001b[0;34m(x, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, boundary, padded, axis)\u001b[0m\n\u001b[1;32m   1173\u001b[0m                                         \u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spectrum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m                                         \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stft'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboundary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mboundary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m                                         padded=padded)\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfreqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZxx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/avalanche/lib/python3.7/site-packages/scipy/signal/spectral.py\u001b[0m in \u001b[0;36m_spectral_helper\u001b[0;34m(x, y, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, mode, boundary, padded)\u001b[0m\n\u001b[1;32m   1834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m     \u001b[0;31m# Perform the windowed FFTs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1836\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fft_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetrend_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnperseg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoverlap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msame_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/avalanche/lib/python3.7/site-packages/scipy/signal/spectral.py\u001b[0m in \u001b[0;36m_fft_helper\u001b[0;34m(x, win, detrend_func, nperseg, noverlap, nfft, sides)\u001b[0m\n\u001b[1;32m   1905\u001b[0m         \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1906\u001b[0m         result = np.lib.stride_tricks.as_strided(x, shape=shape,\n\u001b[0;32m-> 1907\u001b[0;31m                                                  strides=strides)\n\u001b[0m\u001b[1;32m   1908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m     \u001b[0;31m# Detrend each data segment individually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/avalanche/lib/python3.7/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36mas_strided\u001b[0;34m(x, shape, strides, subok, writeable)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0minterface\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'strides'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDummyArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;31m# The route via `__interface__` does not preserve structured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# dtypes. Since dtype should remain unchanged, we set it explicitly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/avalanche/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_EPOCH = 300\n",
    "\n",
    "writer = SummaryWriter(log_dir='runs/crnn_log')\n",
    "\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    training_loss_list = list()\n",
    "    batch_generator = dataset_loader_training.batch_generator()\n",
    "    model.train()\n",
    "    \n",
    "    correct_num = 0\n",
    "    total_num = 0\n",
    "\n",
    "    for (batch, label) in batch_generator:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(torch.cuda.FloatTensor(batch))\n",
    "        loss = F.nll_loss(pred, torch.cuda.LongTensor(label))\n",
    "        loss.backward()\n",
    "        training_loss_list.append(loss.item())\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, pred_label = torch.max(pred, 1)\n",
    "        correct_num += int((pred_label == torch.cuda.LongTensor(label)).sum().cpu().numpy())\n",
    "        total_num += len(label)\n",
    "        \n",
    "    acc_train = correct_num / total_num\n",
    "\n",
    "    testing_loss_list = list()\n",
    "\n",
    "    batch_generator = dataset_loader_testing.batch_generator()\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    correct_num = 0\n",
    "    total_num = 0\n",
    "\n",
    "    for (batch, label) in batch_generator:\n",
    "    #     optimizer.zero_grad()\n",
    "        pred = model(torch.cuda.FloatTensor(batch))\n",
    "        loss = F.nll_loss(pred, torch.cuda.LongTensor(label))\n",
    "#         loss.backward()\n",
    "        testing_loss_list.append(loss.item())\n",
    "    #     optimizer.step()\n",
    "    \n",
    "        _, pred_label = torch.max(pred, 1)\n",
    "        correct_num += int((pred_label == torch.cuda.LongTensor(label)).sum().cpu().numpy())\n",
    "        total_num += len(label)\n",
    "        \n",
    "    acc_test = correct_num / total_num\n",
    "\n",
    "    validation_loss_list = list()\n",
    "\n",
    "    batch_generator = dataset_loader_validation.batch_generator()\n",
    "    \n",
    "    correct_num = 0\n",
    "    total_num = 0\n",
    "\n",
    "    for (batch, label) in batch_generator:\n",
    "    #     optimizer.zero_grad()\n",
    "        pred = model(torch.cuda.FloatTensor(batch))\n",
    "        loss = F.nll_loss(pred, torch.cuda.LongTensor(label))\n",
    "#         loss.backward()\n",
    "        validation_loss_list.append(loss.item())\n",
    "    #     optimizer.step()\n",
    "    \n",
    "        _, pred_label = torch.max(pred, 1)\n",
    "        correct_num += int((pred_label == torch.cuda.LongTensor(label)).sum().cpu().numpy())\n",
    "        total_num += len(label)\n",
    "        \n",
    "    acc_valid = correct_num / total_num\n",
    "    \n",
    "    training_loss_mean = np.mean(training_loss_list)\n",
    "    testing_loss_mean = np.mean(testing_loss_list)\n",
    "    validation_loss_mean = np.mean(validation_loss_list)\n",
    "    \n",
    "    print(datetime.datetime.now().time())\n",
    "    print('#{:03d} [Train: {:0.3f}] [Test: {:0.3f}] [Valid: {:0.3f}]'.format(epoch, training_loss_mean, testing_loss_mean, validation_loss_mean))\n",
    "    print('#{:03d} [Train: {:0.3f}] [Test: {:0.3f}] [Valid: {:0.3f}]'.format(epoch, acc_train, acc_test, acc_valid))\n",
    "    \n",
    "    writer.add_scalar('Loss/Train', training_loss_mean, epoch)\n",
    "    writer.add_scalar('Loss/Test', testing_loss_mean, epoch)\n",
    "    writer.add_scalar('Loss/Valid', validation_loss_mean, epoch)\n",
    "    writer.add_scalar('Acc/Train', acc_train, epoch)\n",
    "    writer.add_scalar('Acc/Test', acc_test, epoch)\n",
    "    writer.add_scalar('Acc/Valid', acc_valid, epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model(torch.cuda.FloatTensor(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor(batch).unsqueeze_(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor = torch.tensor(batch).to(device).float()\n",
    "# input_tensor = input_tensor.transpose(1, 2)\n",
    "# model(input_tensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for batch, label_list in batch_generator:\n",
    "#     print(batch.shape[0])\n",
    "#     plt.figure(figsize=(5, 1))\n",
    "#     plt.plot(label_list)\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.figure(figsize=(6, 18))\n",
    "# #     plt.imshow(batch[0], aspect='auto')\n",
    "#     plt.imshow(batch[0])\n",
    "#     plt.show()\n",
    "    \n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
